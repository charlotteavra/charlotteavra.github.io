<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Charlotte Avra</title>
        
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon_2.ico" />
        
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    
    </head>
    
    
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" style="background-color: #dfe6da;" id="mainNav">
            <div class="container">
                <a class="navbar-brand" href="#">Charlotte Avra</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars ms-1"></i>
                </button>
                
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#portfolio">Projects</a></li>
                        <li class="nav-item"><a class="nav-link" href="#resume">Resume</a></li>
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                    </ul>
                </div>

            </div>
        </nav>
        


        <!-- Project Grid-->
        <section class="page-section bg-light" id="portfolio">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Projects</h2>
                </div>

                <div class="row">
                    
                    <!-- Project 1-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal8">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/bookshelf/5.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robot Librarian</div>
                                <div class="portfolio-caption-subheading text-muted">Nonprehensile manipulation for shelf organization</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 2-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal9">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/buggy/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Linear Control and Path Planning of Autonomous Vehicle</div>
                                <div class="portfolio-caption-subheading text-muted">Controllers for lateral and
                                    longitudinal control of a Tesla Model 3 in Webots</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 3-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal7">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/mouthrobot/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robot Mouth Audio Classification</div>
                                <div class="portfolio-caption-subheading text-muted">Learning human-like tonal inflections 
                                    for studying lip synchronization on a humanoid robot mouth</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 4-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal4">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/sculptbot/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robotic Clay Sculpting</div>
                                <div class="portfolio-caption-subheading text-muted">Pre-trained models for deformable object manipulation</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 5-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal1">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/gastrostomy/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Gastrostomy Skin Level Device</div>
                                <div class="portfolio-caption-subheading text-muted">Design for medical device for patients who 
                                    require enteral feeding focused on long-term durability and patient comfort</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 6-->
                    <div class="col-lg-4 col-sm-6 mb-4 mb-sm-0">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal5">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/electrode/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Electrode Test Setup</div>
                                <div class="portfolio-caption-subheading text-muted">Setup for positioning 3D-printed neural implant probes for stimulation testing</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 7-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal2">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/algae/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Algae Filtration for Kelp Growth</div>
                                <div class="portfolio-caption-subheading text-muted">Abalone-inspired filtration system for addressing issue of diminishing kelp forests off the Pacific coast of the U.S. and Mexico</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 8-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal3">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/globe/1.JPG" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Globe Night Light</div>
                                <div class="portfolio-caption-subheading text-muted">Design of a small, night light desk ornament along with a full manufacturing and assembly process for 500 units</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 9-->
                    <div class="col-lg-4 col-sm-6">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal6">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/redwood/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Redwood Desk Organizer</div>
                                <div class="portfolio-caption-subheading text-muted">Adjustable length desk organizer made of repurposed redwood</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        


        <!-- Resume Timeline -->
        <section class="page-section" id="resume">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Resume</h2>
                </div>
                
                <div class="text-center">
                    <a href="resume/cavra_0103_resume.pdf">
                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                        View as PDF
                        </button>
                    </a>
                </div>  
                
                <div class="text-center">
                    <h1>Experience</h1>
                </div>

                <ul class="timeline">
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/cmu_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Graduate Researcher</h5>
                                <h6 class="subheading">Mechanical and AI Lab | Aug. 2022 - present</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Studying learning for 3-dimensional deformable object manipulation (specifically clay and 
                                    other elasto-plastic materials). Leveraging pre-trained models to predict the 
                                    dynamics of materials and plan trajectories. Employing conventional computer vision techniques to stitch 
                                    and segment 3D point clouds for comprehensive scene representation.  
                                </p> 
                            </div>
                        </div>
                    </li>
                    <li> 
                        <div class="timeline-dot"><img class="rounded-circle img-fluid"></div>
                        <div class="timeline-panel">    
                            <div class="timeline-heading">
                                    <h5>Teaching Assistant</h5>
                                    <h6 class="subheading">Engineering Computation | Aug. 2023 - present</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">
                                    Supported students by grading assignments and conducting office hours for 24-780 Engineering Computation, an 
                                    introductory C++ programming course that delves into fundamental data structures and algorithms emphasizing 
                                    the background algorithms used in modern Computer-Aided Design and Computer-Aided Manufacturing tools. 
                                    Visualization of these algorithms is done using OpenGL.
                                </p> 
                            </div>                                                        
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/medra_logo.jfif" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Robotics Engineer, Intern</h5>
                                <h6 class="subheading">Medra.ai | May - August 2023</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Wrote and tested robotic protocols for handling biology tools designed for humans including collision avoidance 
                                    in a complex environment, used common computer vision techniques for screen reading and manipulation, and tested and debugged in 
                                    simulation (PyBullet) as well as on a physical 6 DOF manipulator. Medra aims to automate repetitive wetlab tasks to accelerate scientific 
                                    discoveries. Unlike current robotic automation in the lab, Medra's goal is to build a robot that can generalize to multiple tasks and 
                                    perform a scientific protocol from beginning to end without human interaction.
                                </p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/nlk_logo.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Mechanical Design Engineer, Intern</h5>
                                <h6 class="subheading">Neuralink | Jan. - June 2022</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Modeled and built a neural implant imaging station and developed an image processing program for documenting the quality 
                                    of the neural implant’s micron-scale threads. My design removed 4 manual steps from the end-of-line process saving time and risk of damaging 
                                    the implant. I also made improvements to the silicon wafer production process by designing and testing a fixture for processing diced wafers. 
                                    My design improved solvent flow in chemical baths, megasonic cleaning, and vapor drying processes, and was more ergonomic for repetitive handling of the 
                                    wafers by microfabrication engineers.
                                </p>                          
                        </div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/rpi_logo_2.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Undergraduate Researcher</h5>
                                <h6 class="subheading">Rensselaer Polytechnic Institute | Aug. - Dec. 2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Improved robustness of Traction Force Microscopy (TFM) image processing program 
                                for images of Schwann cells to more accurately measure cellular traction forces and learn about the development 
                                of the large, soft tumors known as plexiform neurofibromas seen in patients with Neurofibromatosis Type 1 (NF1)
                                </p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/nlk_logo.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Mechanical Design Engineer, Intern</h5>
                                <h6 class="subheading">Neuralink | May - Aug. 2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class = "text-muted">Modeled and built fixture for end-of-line electrode impedance testing implemented 
                                    on neural implant R&D production line, designed parts for machining and injection molding and drafted technical drawings</p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-dot"><img class="rounded-circle img-fluid"></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Manufacturing Technician, Intern</h5>
                                <h6 class="subheading">Neuralink | Jan. - May 2021</h6>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Built neural implant test devices and designed 
                                solutions to improve the production process which included stress testing, soldering, 
                                die bonding, thermal sealing, leak testing, etc.</p></div>
                        </div>
                    </li>
                    <!--
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/crate_barrel_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Stock Associate</h5>
                                <h6 class="subheading">Crate and Barrel | May - Aug. 2019</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Recieved and tracked orders and shipments, prepared online orders for shipping and pickup 
                                </p>
                            </div>
                        </div>
                    </li>
                    -->
                </ul>
                
                <div class="text-center">
                    <h1>Education</h1>
                </div>

                <ul class="timeline">
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/cmu_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Master of Science, Mechanical Engineering</h5>
                                <h6 class="subheading">Carnegie Mellon University | 2022-2024</h6>
                            </div>
                            <div class="timeline-body">  
                                <p class="text-muted">Emphasis: Robotics and Controls</p>
                                <p class="text-muted">Cumulative GPA: 3.99/4.00</p>
                                <p class="text-muted">Achievements: BRIDGE Fellowship (2022 - 2024)</p>
                                <p class="text-muted">Relevant Coursework: Learning for Manipulation, Machine Learning, Deep Learning, C++, Data Structures, 
                                    Modern Control Theory, Computer Vision
                                </p>
                            </div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/about/rpi_logo_2.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Bachelor of Science, Mechanical Engineering</h5>
                                <h6 class="subheading">Rensselaer Polytechnic Institute | 2018-2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Cumulative GPA: 3.55/4.00</p>
                                <p class="text-muted">Achievements: Inventors’ Studio Innovator Award (2021)</p>
                                <p class="text-muted">Relevant Coursework: Elements of Mechanical Design, Modeling and Control
                                    of Dynamic Systems, Electronic Instrumentation, Finite Element Analysis, Fluid Dynamics  
                                </p>
                            </div>
                        </div>
                    </li>
                </ul>
                
                <!-- Resume Skills -->
                    <div class="text-center">
                    <h1>Skills</h1>
                    </div>
                   
                    <div class="row text-left">
                        <div class="col-md-4">
                            <h5 class="my-3">Programming:</h5>
                            <p class="text-muted">Python (incl. OpenCV, SciPy, control), C/C++ (incl. OpenGL), MATLAB & Simulink, 
                                Git, CMake, VSCode</p>
                        </div>

                        <div class="col-md-4">
                            <h5 class="my-3">Robotics & Control Systems:</h5>
                            <p class="text-muted">Design of state feedforward/feedback, optimal, and stochastic controllers and observers 
                                (PID, pole placement, LQR, Kalman filter, MRAC approaches), A* path planning, Hardware (incl. Motors, Actuators, Sensors)</p>
                        </div>

                        <div class="col-md-4">
                            <h5 class="my-3">Design:</h5>
                            <p class="text-muted">Solidworks, Siemens NX, GD&T, Tolerance Stack-up, Material Selection, Design for Machining, 
                                    Design for Plastic Injection Molding</p>
                        </div>

                    </div>

                    <div class="row text-left">
                        <div class="col-md-4">
                            <h5 class="my-3">Machine Learning and Data Analysis:</h5>
                            <p class="text-muted">PyTorch, scikit learn, Tensorflow, keras, MediaPipe, librosa (audio analysis), 
                                    Pandas, Matplotlib</p>
                        </div>

                        <div class="col-md-4">
                            <h5 class="my-3">Prototyping Processes:</h5>
                            <p class="text-muted">SLA and FDM 3D Printing, Laser Cutting, Vacuum Forming, Woodworking</p>
                        </div>
                            
                        <div class="col-md-4">
                            <h5 class="my-3">Manufacturing Processes:</h5>
                            <p class="text-muted">Manual Machining, CNC machining, Injection Molding</p>
                        </div>
                    </div>
            </div>
        </section>
        

        <!-- About-->
        <section class="page-section bg-light" id="about">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">About</h2>
                </div>
                <div class="row">
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/profile.jpg" alt="..." />
                            <p></p>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/charlotteavra/" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/charlotteavra" aria-label="GitHub"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://scholar.google.com/citations?user=wz2YZsYAAAAJ&hl=en&authuser=1" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-8">
                        <p>Hi, thanks for visiting my website! My name is Charlotte and I'm a master's student at Carnegie Mellon in mechanical engineering 
                            with an emphasis in robotics and control systems. I enjoy problem solving, designing, and making, 
                            especially with teams of other passionate engineers and researchers with different backgrounds and interests. My
                            research interest is in robotic manipulation and specifically how we can leverage control algorithms and neural 
                            networks to enable robots to make tasks easier. Last year, I joined the Mechanics and AI Lab at CMU where I'm 
                            currently working on robotic manipulation of deformable objects which has applications in the home and 
                            for the elderly. Outside of class I'm an avid trail runner and enjoy climbing, biking, reading, and crocheting.</p>   
                    </div>
                </div>
            </div>
        </section>
        
        
        <!-- Footer-->
        <footer class="footer py-4">
            <div class="container">
                <div class="row align-items-center">
                    <!--<div class="col-lg-4 text-lg-start">Copyright &copy; Your Website 2022</div>-->
                    <div class="col-lg-4 my-3 my-lg-0">
                        <!--<a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/charlotteavra/" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a>-->
                        <!--<a class="btn btn-dark btn-social mx-2" href="https://github.com/charlotteavra" aria-label="GitHub"><i class="fab fa-github"></i></a>-->
                        <!-- <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="Twitter"><i class="fab fa-twitter"></i></a>
                        <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="Facebook"><i class="fab fa-facebook-f"></i></a> 
                        <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a> -->
                    </div>
                    <div class="col-lg-4 text-lg-end">
                        <!--<a class="link-dark text-decoration-none me-3" href="#!">Privacy Policy</a>
                        <a class="link-dark text-decoration-none" href="#!">Terms of Use</a>-->
                    </div>
                </div>
            </div>
        </footer>
        


        <!-- Portfolio Modals-->
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robot Librarian</h3>
                                        <p class="item-intro text-muted">Nonprehensile manipulation for shelf organization</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/5.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">
                                        Nonprehensile manipulation refers to manipulation not adapted for or not involving antipodal grasping. 
                                        It therefore leverages a wider variety of primitives, such as pushing, sliding, rolling, and tipping 
                                        often allowing for certain object interactions that would otherwise be difficult or even impossible with 
                                        only a grasping primitive. However, this wider variety of primitives can also lead to challenges in 
                                        robotic sensing and control whereby sensitivities like geometry, mass, and friction become more of an issue.
                                    </p>
                                    <p class="text-muted">
                                        Shelves present a unique challenge in robotics as their confined spaces often limit manipulability and 
                                        graspability of the objects inside. These challenges are present in real-world scenarios, especially in 
                                        dynamic environments like kitchens, warehouses, and retail spaces, that demand versatile interactions.
                                    </p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">
                                        In this project, my team and I investigated a shelf organization task and planned efficient robot-pushing 
                                        motions, enabling the reorganization of boxes leaning on a shelf to create space for an additional box. 
                                        Given the first box is placed vertically and the second box is placed at a random angle, the task is to 
                                        plan a nonprehensile push motion to tilt the second box until it is vertical, allowing for a third box to 
                                        be placed. 
                                    </p>

                                    <h5>Method:</h5>
                                    <p class="text-muted">
                                        We first categorized the possible box configurations within the project scope into 4 cases:
                                    </p>
                                    <ul>
                                        <li>
                                            <p class="text-muted">
                                                Case 1: Indicates there is space to insert a third box with no collisions (boxes 1 and 2 
                                                are treated as obstacles)
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 2: Indicates the third box can be placed by treating boxes 1 and 2 as movable rigid
                                                bodies
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 3: Indicates there is no space in the current configuration for the third box to
                                                be placed. The robot must execute a nonprehensile pushing motion to create space for
                                                the third box.
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 4: Indicates all three boxes are in the goal configuration: vertical orientation 
                                                inside the bookshelf.
                                            </p>
                                        </li>
                                    </ul>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Examples of cases 1-4 (from left to right). The blue 
                                        highlighted region indicates a space large enough to place the third box. 
                                    </p>

                                    <p class="text-muted">
                                        Fig. 2 illustrates the general pipeline of our framework. We learned a classifier that determines 
                                        whether of not a box can be successfully placed in a target location given a depth image of the 
                                        environment. The classifier was trained on depth images collected from simulation of the scene 
                                        initialized with different box configurations. The images were automatically labeled by first 
                                        checking if all three boxes were within the boundaries of the shelf at the end of the robot
                                        action, and whether the two initially placed boxes had moved greater than a threshold distance.      
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/flowchart.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: General framework of our approach 
                                    
                                    <h6>Sampling-based Approach:</h6>
                                    <p class="text-muted">
                                        I explored our first approach for planning an efficient robot-pushing motion which involved uniform random 
                                        sampling of contact points on the surface of the box, simulating a push from that location, and evaluating 
                                        the quality of the push. This approach is uninformed, meaning it does not exploit any specific knowledge of 
                                        the goal and only makes decisions based on what is immediately visible in the current state. It therefore 
                                        also does not guarantee a successful push given a certain box configuration. However, it provides a starting 
                                        point for later, more informed approaches.   
                                    </p>

                                    <p class="text-muted">
                                        We capitalize on the assumption that a physics simulator (PyBullet) can be used within the planning loop to 
                                        evaluate pushing actions. I first randomly sampled 200 contact points on the inclined box. An end-effector 
                                        pose was then defined for each contact point using the (x,y,z) coordinates of the contact point and 
                                        constraining the orientation to vertical with respect to the world frame as shown in Fig. 3. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Sampling contact points from surface of angled box and defining end-effector 
                                        pose based on sampled point</p>

                                    <p class="text-muted">
                                        The corresponding joint angles were then computed using <code>p.calculateInverseKinematics()</code>. If the current
                                        joint configuration was in collision or the IK failed, a zero score was assigned to the attempted push. 
                                        Conversely, if the configuration was collision-free, the push was executed using a simple motion primitive defined as 
                                        a straight-line movement along the world frame x-axis, opposing the x normal of the sampled point, effectively 
                                        pushing against the surface of the box. The heuristic used to evaluate the push was a score proportional to the distance 
                                        between the final orientation of the box, <code>p</code>, and the goal vertical orientation, <code>q</code>, both 
                                        represented as quaternions:
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/score.png" alt="..." />

                                    <h6>Search-based Approach:</h6>
                                    <p class="text-muted">
                                        Unfortunately, the previous approach does not exploit any specific knowledge of the goal to make decisions. 
                                        To take a more informed approach, I also attempted a search-based method. Similar to the previous appraoch, 
                                        I use the physics simulator within the planning loop.
                                    </p>

                                    <p class="text-muted">
                                        I first discretized the surface of the box into a grid of points as shown in Fig. 4. I then used an A* 
                                        search to find the optimal path to the goal node, which represents the physical location on the box 
                                        from which to initiate a push. I used a heuristic similar to the scoring function used in the 
                                        sampling-based approach to define a "cost-to-go" estimation such that nodes with lower cost-to-go 
                                        estimations will be explored earlier. <code>p</code> is again the final orientation of the box and 
                                        <code>q</code> is the goal vertical orientation, both represented as quaternions: 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/heuristic.png" alt="..." />

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/grid.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Discretizing box surface into grid of contact points</p>
                                    
                                    <p class="text-muted">
                                        I again used a straight-line push motion primitive when evaluating push actions but I changed the 
                                        orientation of the end-effector to be parallel with the surface of the box instead of vertical with 
                                        respect to the world frame. The discretized points are organized as a <code>numpy.meshgrid</code>. 
                                        In the planning loop, the successors of each node are simply the eight neighbors of the current node.  
                                    </p>

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">
                                        We ran 50 randomly generated trials and recorded the average plannning time and success rate. In each 
                                        trial, the first box is placed vertically, and the second box is placed at a random angle between 
                                        <code>(-pi/4, -pi/6)</code> and <code>(pi/6, pi/4)</code>. We then run 100 steps of the simulation to allow 
                                        the boxes to settle. 
                                    </p>

                                    <h6>Sampling-based Approach:</h6>
                                    <p class="text-muted">
                                        The uninformed sampling approach achieved a 20% success rate across all 50 trials, and an average planning 
                                        time of 25.1 seconds.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/sample_gif_success.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Example of a successful push action (score = 0.998)</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/sample_gif_fail.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Example of a failed push action (score = 0.794)</p>
                                    
                                    <h6>Search-based Approach:</h6>
                                    <p class="text-muted">
                                        The success rate increased to 78% for the search-based approach and the planning time reduced by approximately 
                                        20 seconds, with an average planning time of 5.08 seconds per trial, compared to the sampling-based approach.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/search_gif_2.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Example of a successful push action (h(s) = 0.0042)</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">
                                        Prehensile manipulation can set artificial limits on the range of tasks a robot can perform. Dynamic nonprehensile 
                                        manipulation, which leverages a wider variety of primitives such as pushing, sliding, rolling, and tipping, exploits 
                                        dynamics, thereby allowing for certain object motions that would otherwise be difficult or even impossible with only 
                                        a grasping primitive. We explore nonprehensile manipulation in a confined shelf environment where various tasks, 
                                        such as inserting and organizing are difficult when constrained to prehensile manipulation only. We find that 
                                        our search-based approach performs significantly better than our uninformed sampling approach by increasing success 
                                        rate and decreasing planning time. For future work, a more sophisticated heuristic could be investigated as well 
                                        as a more sophisticated push motion primitive.   
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/7.png" alt="..." />
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal9" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Linear Control and Path Planning of Autonomous Vehicle</h3>
                                        <p class="item-intro text-muted">Controllers for lateral and
                                            longitudinal control of a Tesla Model 3 in Webots</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">There have been significant advancements in autonomous vehicle technology in 
                                        recent years. As more self-driving vehicles are being developed and tested, there is an increasing 
                                        need for robust controllers and efficient path planning algorithms to ensure safe and reliable 
                                        autonomous operation.</p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">The goal of this project was to:</p>
                                    <ul>
                                        <li>
                                            <p class="text-muted">
                                                Develop an optimal controller for lateral and longitudinal control of a simulated Tesla Model 3 in 
                                                Webots making use of the kinematic bicycle model, shown in Fig. 1, for studying the vehicle's dynamics.
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Implement the A* path planning algorithm to re-plan the trajectory given a second car is on the track 
                                                that you must pass. 
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Predict the global position and heading from observable states using an extended Kalman filter (EKF) 
                                                given some localization information is missing.
                                            </p>
                                        </li>
                                    </ul>

                                    <p class="text-muted">
                                        The car drives around a track with the same geometry as the CMU buggy course. Buggy is 
                                        a time-honored CMU tradition where students race human-powered, small, aerodynamic vehicles around Schenley 
                                        Park's Flagstaff Hill. More information on it can be found <a href="https://www.cmu.edu/buggy/">here</a>. Like 
                                        buggy, the goal of the controller is to complete the 0.84 mile course minimizing both time and deviation from 
                                        the course.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Kinematic bicycle model</p>

                                    <h5>Method:</h5>
                                    <h6>Discrete-Time Linear Quadratic Regulator (LQR):</h6>
                                    <p class="text-muted">I first implemented an infinite horizon discrete-time Linear Quadratic Regulator (LQR) controller 
                                        for lateral control of the vehicle and a simple PID control for longitudinal control. The error-based linearized state 
                                        space model for the lateral dynamics is given by:  
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lateral_dynamics.png" alt="..." />

                                    <p class="text-muted">where e<sub>1</sub> is the cross-track error, or distance to the center of gravity of the 
                                        vehicle from the reference trajectory, and e<sub>2</sub> is the heading error, or orientation error of the vehicle 
                                        with respect to the reference trajectory.</p>

                                    <p class="text-muted">The longitudinal dynamics are given by:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/long_dynamics.png" alt="..." />

                                    <p class="text-muted">Given the <code>A</code> and <code>B</code> matrices, I then discretize the lateral state space model 
                                        using <code>signal.cont2discrete(system=(A,B,C,D))</code>, where <code>C</code> is an identity matrix with the same size as 
                                        <code>A</code>, and <code>D</code> is a matrix of zeros with the same size as <code>B</code>. I then tune 
                                        my <code>Q</code> and <code>R</code> matrices before solving the discrete-time algebraic Riccati equation (ARE) using 
                                        <code>linalg.solve_discrete_are(A, B, Q, R)</code>. Lastly, we can define the LQR gain as:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_gain.png" alt="..." />
                                    
                                    <p class="text-muted">and compute the control outputs using:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/input.png" alt="..." />
                                    
                                    <h6>PID Controller for Longitudinal Control:</h6>
                                    <p class="text-muted">For the PID controller, I define a minimum and maximum speed as <code>speed_min</code> and <code>speed_max</code>, 
                                        chosen emperically. I then compute the curvature of the track at the current location using the closest waypoint and a waypoint at a 
                                        certain look ahead distance (also defined emperically), and set the <code>ideal_speed</code> of the car to be 
                                        <code>max(speed_min, min(speed_max, (1/curvature)))</code>. Then, the <code>speed_error</code> is simply <code>(ideal_speed-current_speed)</code>, 
                                        the integrated speed error is <code>total_speed_error</code>, and the derivative of the speed error is <code>speed_error_rate</code>. Lastly, 
                                        I tune my PID gains <code>Kp</code>, <code>Ki</code>, and <code>Kd</code>, and compute the output throttle: 
                                        <code>F = (Kp*speed_error) + (Ki*total_speed_error) + (Kd * speed_error_rate)</code>.
                                    </p>
                                    
                                    <h6>A* Path Planning for Obstacle Avoidance:</h6>
                                    <p class="text-muted">Obstacle avoidance is crucial in autonomous vehicle control. In the previous example, there was only 
                                        one vehicle on the track, but if another is introduced, we must determine how to safely overtake it. For this, I take a 
                                        very simplified approach where I assume we will overtake the other vehicle in the straight-away so we can re-plan the 
                                        trajectory once and follow the new trajectory open-loop. </p>

                                    <p
                                     class="text-muted">To re-plan the trajectory, we can first compute the H, G, and F values of the known start node, and 
                                        add it to the open heap queue using <code>heappush(open_list, start_node)</code>. I then implement a loop to continue 
                                        searching the map until the shortest path to the end node is reached. Starting at 7:35 in 
                                        <a href="https://youtu.be/-L-WgKMFuhE?si=lchF4JSsQKhJp_MY&t=455">this video</a>, there is a great explanation of this 
                                        search.
                                    </p>
                                    
                                    <h6>Extended Kalman Filter Simultaneous Localization and Mapping (EKF SLAM)</h6>
                                    <p class="text-muted">Finally, in the previous implementations, we are given the global coordinates of the 
                                        course trajectory, but these are not always available in real-world scenarios. Localization information from GPS 
                                        could be missing or inaccurate in tunnels or in close proximity to tall infrastructure. In this case, we do not 
                                        have direct access to the global position, <code>X</code> and <code>Y</code>, and heading, <code>psi</code>, and 
                                        must estimate them from observable states in the vehicle
                                        frame and range and bearing measurements of map features. I used an extended Kalman filter (EKF) for predicting the 
                                        distances to map coordinates whose global coordinates are provided.
                                    </p>

                                    <p class="text-muted">Let the state vector be:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/state_vector.png" alt="..." />
                                    <p class="text-muted">where there are <code>n</code> map features at global position <code>m</code>.</p>

                                    <p class="text-muted">
                                        The ground truth of these map feature positions are static but unknown, meaning they will not move but 
                                        we do not know where they are exactly. However, the vehicle has both range and bearing measurements 
                                        relative to these features:
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/measurement_system.png" alt="..." />

                                    <p class="text-muted">
                                        We can assume a closed-form expression for the predicted state as a function of the previous state, and 
                                        the measurement can be a function of the state and the measurement noise (more info 
                                        <a href="https://www.mathworks.com/help/driving/ug/extended-kalman-filters.html">here</a>):
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/x_y.png" alt="..." />

                                    <p class="text-muted">
                                        Then, I computed <code>F</code>, the Jacobian of the predicted state with respect to the previous state, 
                                        and <code>H</code>, the Jacobian of the measurement with respect to the state. I computed these matrices 
                                        by hand.
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/F_H.png" alt="..." />

                                    <p class="text-muted">
                                        With <code>n = 8</code> map features, <code>F</code> is a <code>(3 + 2n, 3 + 2n) = (19, 19)</code> size 
                                        array, and <code>H</code> is a <code>(2n, 3 + 2n) = (16, 19)</code> size array. 
                                    </p>

                                    <p class="text-muted">
                                        I then iteratively "predicted and corrected", predicting the state and error covariance, then updating both 
                                        estimates after computing the Kalman gain.  
                                    </p>
                                    
                                    <h5>Evaluation:</h5>
                                    <h6>Discrete-Time Linear Quadratic Regulator (LQR):</h6>
                                    <p class="text-muted">The tuned LQR controller completed the track in approximately 120 seconds with a 
                                        maximum deviation of 4.69 meters and an average deviation of 0.41 meters as shown in Fig. 2.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Performance of linear quadratic regulator</p>

                                    <h6>A* Path Planning for Obstacle Avoidance:</h6>
                                    <p class="text-muted">I evaluated the performance of my A* implementation with a few arbitrary costmaps as shown 
                                        in Fig. 3, to ensure it was behaving the way I intended. These maps do not relate to the track but provide 
                                        a more general evalutaion of the function.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/astar_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Performance of A* path planning algorithm on arbitrary costmaps</p>
                                    
                                    <p class="text-muted">Fig. 4 shows the performance on the actual track where the yellow in the plot represents obstacles. 
                                        It can also be seen that we rather naively represent the second car as a large static obstacle. This means 
                                        this approach works only when the second car moves at the same velocity each race, and that once we pass the 
                                        second car in the straightaway, we must stay ahead of it to avoid collision.  
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/astar_track_performance.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/gif_1.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Performance of A* path planning algorithm on a section of the track (top) and 
                                        a GIF of the resulting simulation showing the overtaking of the second vehicle (bottom)
                                    </p>

                                    <h6>Extended Kalman Filter Simultaneous Localization and Mapping (EKF SLAM)</h6>
                                    <p class="text-muted">
                                        I tested the performance of my EKF implementation on an arbitrary trajectory as shown in Fig. 5, before 
                                        integrating it into my Webots simulation. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/ekf_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Performance of EKF implementation on arbitrary trajectory</p>
                                    
                                    <p class="text-muted">The tuned LQR controller with EKF SLAM completed the track in approximately 121 seconds. 
                                        The maximum deviation increased slightly from 4.69 meters when global position was known to 5.10 meters using 
                                        global position predictions and the average deviation from 0.41 meters to 0.84 meters as shown in Fig. 6.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_ekf_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Performance of EKF implementation using previously described LQR 
                                        controller on Buggy course in Webots</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">In conclusion, studying controllers and path planning for autonomous vehicles is essential 
                                        to address the evolving challenges and opportunities in the field and to pave the way for the safe and 
                                        widespread adoption of autonomous vehicle technology.</p>
                                    
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robotic Clay Sculpting</h3>
                                        <p class="item-intro text-muted">Pre-trained models for deformable object manipulation</p>
                                        <p class="item-intro text-muted">
                                            <a href="https://arxiv.org/abs/2309.08728">[PAPER]</a> | 
                                            <a href="https://sites.google.com/andrew.cmu.edu/sculptbot/home">[WEBSITE]</a>
                                        </p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/1.jpg" alt="..." />

                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">Deformable objects are prevalent in tasks related to cooking, manufacturing, medical procedures, 
                                        and more. It is therefore necessary to develop robotic systems that can effectively handle and manipulate 
                                        deformable objects before being deployed in these environments. However, due to their plastic behavior, 
                                        deformable objects have complex dynamics. State estimation for deformable objects is also more difficult 
                                        than for rigid objects that retain a constant shape. Therefore, we present a system that leverages pre-trained point cloud 
                                        reconstruction models to learn a latent dynamics model for 3D deformable object manipulation.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Deformable objects prevalent in cloth manipulation, cooking,
                                        computer manufacturing, and medical procedures
                                    </p>
                                    
                                    <h5>Objectives:</h5>
                                    <p class="text-muted">We chose the task of robotic sculpting of modeling clay to develop a system that can learn
                                        a latent representation of the object and plan trajectories for sculpting a certain goal shape. The goal shape 
                                        would be provided to the system as point cloud data.</p>
                                    
                                    <h5>Method:</h5>
                                    <p class="text-muted">
                                        Our method for sculpting the modeling clay primarily consists of 3 modules:</p>
                                        <ul>
                                            <li>
                                                <p class="text-muted">Perception: We represent the environment as point cloud data and isolate the 
                                            modeling clay using position and color-based cropping.</p>
                                            </li>
                                            <li>
                                                <p class="text-muted">Tokenizer: We leverage the tokenizer from <a href="https://arxiv.org/abs/2111.14819">Point-BERT</a>, a pre-trained model 
                                                    that generalizes the concept of BERT (a family of large language models) to 3D point clouds.</p>       
                                            </li>
                                            <li>
                                                <p class="text-muted">Dynamics Model: We train both a simple physics approximator and a DGCNN token predictor model. The predicted 
                                                    tokens from the DGCNN along with the predicted centroid point cloud from the physics approximator are then passed through the 
                                                    Point-BERT dVAE decoder to reconstruct the full dense predicted next state point cloud. 
                                                </p>
                                            </li>
                                        </ul>
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: A visualization of our method for scultping modeling clay that primarily consists of 3 modules</p>
                                    
                                    <h6>Perception:</h6>
                                    <p class="text-muted">We calibrated four workspace cameras and used a global registration algorithm (RANSAC) to first roughly align the point clouds 
                                        taken by the cameras. We then used iterative closest point (ICP) algorithm for refinement.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: A top-down view of workspace with four RGBD cameras (left) and a visualization of the global and local 
                                        registration algorithms we used for point cloud alignment 
                                    </p>

                                    <p class="text-muted">The aligned point clouds were then pre-processed to produce a more quality representation. Due to the top-down images we 
                                        captured from the RGBD cameras, the base of the clay was occluded. We first perform a simple position-based crop to isolate the elevated stage, 
                                        and use color-based cropping in the LAB colorspace to isolate the clay from the table. We then extract the points closest to the base of the
                                        clay (by indexing points below a z threshold). We use this shape outline to crop a plane of points to form the bottom of the clay. We combine this 
                                        base plane with the original cloud to form a fully enclosed point cloud shell. This processing is based on the assumption that the clay
                                        is always resting on the elevated stage, thus the base of the clay will be at that z-position. Once we have the clay shell, we downsample the 
                                        point cloud to 2048 points.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Preprocessing pipeline for point clouds (from left to right) original point cloud, position-based cropping,
                                        color-based cropping, add base plane, downsample 
                                    </p>

                                    <h6>Point-BERT Tokenizer:</h6>
                                    <p class="text-muted">We leverage the tokenizer from Point-BERT, a pre-trained model that generalizes the 
                                        concept of BERT (a family of large language models) to 3D point clouds, to learn a latent representation of the input point cloud data. Point-BERT is 
                                        pre-trained on the ShapeNet dataset, a collection of 3D point clouds of 3,135 common categories.  
                                    </p>

                                    <h6>Dynamics Models:</h6>
                                    <p class="text-muted">Within this latent space, our simple physics approximator learns to propagate the point cloud based on a grasp action and output 
                                        a next-state centroid. This dynamics approximator is very light weight, moving the points that are inliers in the approximated gripper trajectory 
                                        mesh based on the grasp pose.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Light-weight physics-based approximator learns next-state centroid given an initial state and a grasp action 
                                    </p>

                                    <p class="text-muted">Based on centroid movement, we then predict how features of each point cloud cluster change and deform and output next-state 
                                        discrete tokens. To do this we train a <a href="https://www.sciencedirect.com/science/article/pii/S0893608018302636">DGCNN</a> token predictor 
                                        model to predict how each cluster’s point token changes given the grasp action. These next-state discrete tokens are then decoded by the Point-BERT 
                                        decoder to output a next-state point cloud in real space.</p>
                                    
                                    <h6>Data Collection:</h6>
                                    <p class="text-muted">To train alternate versions of the dynamics models, we collect two separate datasets on a Franka Emika Panda manipulator: a random
                                        action dataset and human demonstratino dataset. The random action dataset was collected by randomly sampling action parameters and executing the 
                                        generated grasps. A point cloud of the clay was collected before and after each grasp. The human demonstration dataset collected using kinesthetic 
                                        teaching where the human controlled the end-effector position, rotation and the distance between the fingertips. A point cloud of the clay was again 
                                        collected before and after each grasp.
                                    </p>

                                    <p class="text-muted">During initial data collection, we were noticing the robot grippers would unintentionally pick up the clay rendering it difficult 
                                        to collect more difficult to collect data in the same trajectory. We designed a small part that would act as an anchor to hold the middle of the 
                                        clay down to the elevated stage.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: CAD model of the data collection setup with the Franka manipulator, elevated stage for the clay (initialized to
                                        a cylinder), and a small 3D printed screw that acted as an anchor to keep the center relatively restrained to the stage
                                    </p>

                                    <p class="text-muted">We found that although this slighlty restricted motion of the grippers, it aided greatly in both the random action and human 
                                        demonstration data collection processes.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Robot grippers picking up the clay in some orientations when collecting data for the random action dataset (left)
                                        and the clay being held down by a center anchor (right)
                                    </p>

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">We plan action trajectories using model predictive control (MPC) and evaluate the results of different sculpting tasks. The dynamics 
                                        model trained on the human demonstration dataset had a 16.1% lower mean chamfer distance than the dynamics model trained on  the random action dataset.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: Shapes our human demonstrator was able to achieve with the robot in guiding mode (left) and the shapes output by the 
                                        human demonstration trained dynamics model combined with MPC and geometric sampling (right)
                                    </p>
                                    
                                    <div class="text-center">
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-xmark me-1"></i>
                                        Close Project
                                    </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robot Mouth Audio Classification</h3>
                                        <p class="item-intro text-muted">Learning human-like tonal inflections 
                                            for studying lip synchronization on a humanoid robot mouth</p>
                                        <p class="item-intro text-muted">
                                            <a href="https://github.com/charlotteavra/mouth_robot">[CODE]</a>
                                        </p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">This project attempts to quantify the human-likeness of sound produced by 
                                        a humanoid robot. Humanoid robots are often designed to interact with humans in various settings, 
                                        such as homes, workplaces, or public spaces. Therefore, human-like sound allows robots to communicate 
                                        with humans in a more natural and intuitive way improving user interaction and social integration. 
                                        The robot we used has no speakers and produces sound from a variable pitch pneumatic
                                        sound generator and resonance tube deformed by a series of servo motors along its length as shown in 
                                        Fig. 1.   
                                    </p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">Our goal was to quantify the human-likeness of audio produced by the robot such 
                                        that we can use this metric to rate each sound and iteratively update the robot's hardware to 
                                        produce a more human-like sound.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Humanoid robot mouth used for this project</p>

                                    <h5>Audio Processing:</h5>
                                    <p class="text-muted">The way in which data is processed before inputting it into any kind of 
                                        machine learning or deep learning model is fundamental. In the domain of audio, there are 
                                        several concepts that aid in data pre-processing.
                                    </p>

                                    <h6>Mel Spectrograms:</h6>
                                    <p class="text-muted">Digital representations of audio signals most often begin as the relationship 
                                        of amplitude and time. However, to extract useful information from these signals, a Fourier 
                                        transform can be applied to decompose a signal into its individual frequencies and their 
                                        amplitudes and therefore convert from the time to frequency domain.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Example audio signal represented by signal amplitude in 
                                        the time domain as well as in the frequency domain after applying a Fourier transform 
                                        (Image Source: <a href="https://insightincmiami.org/data-visualization-using-the-fourier-transform/">insightincmiami.org</a>)</p>
                                        
                                    <p class="text-muted">Most speech and music signals are non-periodic. This means that to represent 
                                        these signals in the frequency domain, a Fast Fourier Transform (FFT) is performed over several 
                                        windowed segments of the signal. What results is called a spectrogram. Spectrograms are 
                                        visualizations or figures of audio that represent the spectrum of frequencies over time for 
                                        an audio recording.
                                    </p>

                                    <p class="text-muted">If frequency is converted from Hertz to the Mel Scale, a representation of 
                                        frequency that mimics the perception of sound by humans and hence why it is used often in 
                                        machine learning, the spectrogram is called a Mel Spectrogram.
                                    </p>    
    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Example Mel Spectrograms of a human (left) and our mouth 
                                        robot producing a high-level tone</p>

                                    <h6>Mel Frequency Cepstral Coefficients:</h6>

                                    <p class="text-muted">The Mel Frequency Cepstrum (MFC) is a discrete cosine transformation (DCT) 
                                        on the log of the magnitude of the Fourier spectrum which is obtained by applying a Fourier 
                                        transform on the time signal. MFCC’s are coefficients that collectively make up an MFC. MFCC's 
                                        visually represent features of the audio remarkably well and therefore can be input into a 
                                        convolutional neural network for classification.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Example Mel Frequency Cepstral Coefficients of a human  
                                        producing a dipping tone (left) and a falling tone (right)</p>

                                    <h5>Method:</h5>
                                    <h6>Data Collection:</h6>
                                    <p class="text-muted">We first collected several thousand audio recordings from the robot. To construct the 
                                        dataset, we first structured each data point as a sequence of 3 varied pitches with a 
                                        repeating open and closed actuation of the mouth. We then populated each sequence with an 
                                        initial guess of the relative pitch values of a high-level, rising, dipping or a falling tone, 
                                        then labeled them as such. We executed these tones on the robot and recorded the audio output.
                                        There was an equal class distribution in these data.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Data collection and automatic labeling process</p>

                                    <p class="text-muted">Data for human audio recordings, which we also experimented with, were 
                                        sourced from the <a href="https://tone.lib.msu.edu/">Tone Perfect database</a>. Tone Perfect 
                                        includes 9,840 audio files representing 410 monosyllabic sounds in Mandarin 
                                        Chinese each recorded from six speakers using four different tones: high-level tone, rising 
                                        tone, dipping tone, and falling tone. We used 4,500 of these audio files. 
                                    </p>

                                    <h6>Data Augmentation:</h6>
                                    <p class="text-muted">The iteration time for updating the robot hardware to test different 
                                        configurations in the hopes of obtaining a more human-like sound would be long. Therefore, 
                                        we chose to augment data from the robot in different ways to speed up iteration cycles.
                                    </p>

                                    <p class="text-muted">1. The data augmentation process included first compressing the audio 
                                        signal from three seconds to one second. 
                                    </p>

                                    <p class="text-muted">2. Due to a shortened wavelength when compressing the audio signal, this 
                                        resulted in an increased pitch. Each audio signal was then pitched down by 1.5 octaves to 
                                        return to its original pitch. This was based on human perception of the original pitch and 
                                        was not quantitatively computed.
                                    </p>
                                       
                                    <p class="text-muted">3. The pitch shifting operation caused the decibel level of each 
                                        signal to be reduced significantly so each signal was increased by 15 decibels.
                                    </p>
                                        
                                    <p class="text-muted">4. The pitch shifting operation caused the decibel level of each signal to 
                                        be reduced significantly so each signal was increased by 15 decibels.
                                    </p> 

                                    <p class="text-muted">5. A mic pop at the beginning of each audio signal was removed and 
                                        A 0.25 second fade in/fade out effect was added to each audio signal to mimic the change 
                                        in volume that may occur when a human opens and closes their mouth.
                                    </p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/7.png" alt="..." />
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/8.png" alt="..." /> 
                                        </div>
                                    </div>

                                    <p class="item-intro text-muted">Fig. 6: Waveplots of example raw robot audio signal and 
                                        augmented audio signal
                                    </p>

                                    <p class="text-muted">60 MFCC’s for each audio signal were then computed using the librosa sound 
                                        processing library, zero padding was applied to ensure square format, and then they were 
                                        input into the model. 
                                    </p>

                                    <h6>Model Architecture:</h6>
                                    <p class="text-muted">The CNN architecture we used was adapted from 
                                        <a href="https://github.com/adhishthite/sound-mnist">sound-mnist</a> and has 3 
                                        convolution layers with relu activation and batch normalization after each layer. Then a 
                                        max pooling layer and dropout followed by 3 fully connected layers the last one having 
                                        softmax activation.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: CNN architecture used for all experiments</p>
                                    
                                    <p class="text-muted">The goal of the model was to learn from MFCC's based on audio produced by humans, 
                                        specifically 4,500 signals from the Tone Perfect database, and be tested on 1,120 
                                        MFCC's based on audio produced by the robot to determine if the robot sounds were human-like. 
                                    </p>

                                    <p class="text-muted">We wanted the model to generalize enough such that it could maintain high 
                                        accuracy given a validation set of MFCC's that may look very different from what it was trained 
                                        on, but still reflect the human-likeness of the validation set through its softmax output 
                                        predictions: high value predictions for human-like sounds and low value predictions for 
                                        non-human-like sounds. This, however, proved to be a difficult goal. 

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">We first show here four examples of the MFCC’s that the model learned and 
                                        was tested on. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/11.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/12.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/13.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: MFCC's visualizing four tonal inflections produced by 
                                        a human (left) and our mouth robot (right)
                                    </p>
                                    
                                    <h6>Training and Testing on Robot MFCC's:</h6>
                                    <p class="text-muted">We first split the 1,120 robot MFCC's with 80% (880 MFCC's) for training and 
                                        20% (220 MFCC's) for validation. When trained on 880 robot audio signals, the validation 
                                        accuracy was 92.4% as shown in Fig. 9. This proves the model can successfully classify robot 
                                        MFCC's when trained on them with high accuracy validating the choice of model architecture 
                                        for the remaining experiments.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/15.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: High model performance when trained and validated using only
                                        robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, Epochs = 30, 
                                        Batchsize = 20)</p>

                                    <h6>Training on Human MFCC's and Testing on Robot MFCC's:</h6>
                                    <p class="text-muted">For this experiment, we used all 4,920 human MFCC's for training and 
                                        validated the model using all 1,120 robot MFCC's. This time, the model had difficulty 
                                        recognizing features in the validation set and yielded a 25.00% accuracy.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/17.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 10: Poor model performance when trained on human MFCC's and 
                                        validated using robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, 
                                        Epochs = 50, Batchsize = 20)</p>

                                    <p class="text-muted">At around 33 epochs, the validation loss became greater than the training 
                                        loss, as shown in Fig. 10, which provides an indication of overfitting in the training data. 
                                        The training accuracy was above 99% which is another indication the model began to overfit 
                                        the training data and is therefore unable to generalize. This resulted in heavy overprediction 
                                        of class 3.
                                    </p>

                                    <p class="text-muted">The model was then retrained on the human audio signals with 30 epochs, as 
                                        shown in Fig. 11, in an attempt to limit overfitting. The model performed slightly better with 
                                        an accuracy of 25.71% and was able to predict tones beyond class 3 but was still largely 
                                        overpredicting class 3.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/16.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 11: Poor model performance when trained on human MFCC's and 
                                        validated using robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, 
                                        Epochs = 30, Batchsize = 20)</p>

                                    <h5>Conclusion:</h5>
                                    <h6>Future Work:</h6>
                                    <p class="text-muted">As this was only a semester long project, we weren't able to run all the 
                                        experiments we wanted to. For future work, we would look more into updating the hardware 
                                        by either replacing the bagpipe reed with a sound generation mechanism with a larger span of 
                                        possible pitches or replacing our air pump with one with a larger span of possible speeds. This 
                                        would all be in an attempt to manually produce a sound that sounds more human-like, before 
                                        studying which features a neural network uses to classify each tone. 
                                    </p>

                                    <p class="text-muted">A deep reinforcement learning approach could also be attempted. As the goal 
                                        of this algorithm is to maximize the accumulated reward, the agent can ignore possible 
                                        limitations in hardware as it is solely identifying the best possible combination of actions 
                                        to determine the optimal policy. 
                                    </p>

                                    <p class="text-muted">The reward estimation could be directly correlated to the softmax predictions 
                                        of the model, wherein the predicted value of the desired tone is used as the reward for the 
                                        agent.
                                    </p>

                                    <h6>Final Word:</h6>
                                    <p class="text-muted">This is certainly a unique problem and this project is far from complete. 
                                        Although the results yielded accuracy lower than we had hoped, we were happy to have chosen
                                        a challenging project and learned from the time we spent working on it.  
                                    </p>


                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Gastrostomy Skin Level Device</h3>
                                        <p class="item-intro text-muted">Design for medical device for patients who require enteral 
                                            feeding focused on long-term durability and patient comfort</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/1.jpg" alt="..." />
                                       
                                    <h5>Introduction:</h5>
                                    <p class="text-muted">About half a million children and adults in the United States rely on feeding 
                                        tubes everyday. There are over 350 conditions and diseases in which enteral feeding may be necessary. 
                                        Enteral nutrition, or the method of delivering nutrition directly to the stomach 
                                        or small intestine, is required for anyone who cannot meet their nutritional needs by oral intake 
                                        but have a functional gastrointestinal tract. The use of enteral nutrition can be due to several factors. 
                                        Dysphagia, a difficulty to swallow, affects one-third of patients with Parkinson’s disease (PD).  
                                        Also, maintaining nutritional health can be difficult for people with cystic fibrosis (CF) as well as people 
                                        undergoing chemotherapy where a feeding tube can offer much needed nutrients and calories.
                                    </p>

                                    <p class="text-muted">Skin level devices, often referred to as “G-buttons” or “Gastrostomy buttons” 
                                        are medical devices designed for enteral feeding. They are inserted through a surgical incision 
                                        in the stomach called a gastrostomy and interface with a gastrostomy tube through which nutritional 
                                        formula flows. The quality of devices like the one shown in Figure 1 is crucial to ensuring patient 
                                        safety and comfort. Further research and development in this field will allow for customer feedback 
                                        of current products to be addressed. Innovation in the medical device industry in general is therefore 
                                        crucial in ensuring that the medical needs of all patients are met. 
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Schematic of Gastrostomy Skin Level Device</p>

                                    <h5>Design:</h5>
                                    <h6>Research and Benchmarking:</h6>
                                    <p class="text-muted">Following the human-centered design process I first researched and 
                                        subsequently defined the problem to provide a basis for development. As part of the research 
                                        phase, I developed a customer requirements table along with accompanying functional requirements, 
                                        technical interpretations, technical specifications, and metrics. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/15.png" alt="..." />
                                    <p class="item-intro text-muted">Table 1: Table of customer requirements each assigned a number 
                                        and color coded based on priority</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/16.png" alt="..." />
                                    <p class="item-intro text-muted">Table 2: Table with functional requirements of the product relating 
                                        to each of the customer requirements (Table 1) as well as a technical interpretation of the 
                                        requirement</p>

                                    <p class="text-muted">For product benchmarking, a thorough assessment of current technologies 
                                        that address similar customer requirements was then completed. 
                                        These technologies included products for purchase as well as several U.S. patents.
                                    </p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Current Products on the Market: AMT MiniONE® “Family 
                                        of G-Tubes”</p>
                                        
                                    <p class="text-muted">U.S. Patent Application Pub. No. 2006/0052752 was worth noting as it 
                                        provides a concept design for a gastrostomy button similar to the mentioned products but with 
                                        some improvements. The design offers a non-balloon internal bolster approach to allow for longer 
                                        wear time as gastrostomy buttons with silicone balloons must be replaced approximately every 
                                        three months to prevent rupture due to the concentration of hydrochloric acid (HCI) present in 
                                        gastric fluid. The design also includes two sets of silicone pleats that act as springs to 
                                        stabilize the port. An image displaying two views of the patented design are shown below in 
                                        Figure 3.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: SLD design involving silicone “pleat” stabilizers and 
                                        non-balloon internal bolster (U.S. Patent No. 8,951,232)</p>
                                    
                                    <h6>Prototyping:</h6>
                                    <p class="text-muted">The design process focused on addressing all customer needs especially those 
                                        involving patient safety and reliability. As part of the design process, several 3D printed 
                                        prototypes of the design were developed, tested, and iterated upon. One of the first prototypes
                                        is shown in Fig. 4. 
                                    </p>    

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Testing of 3D printed prototype for insertion ability 
                                        with thin metal rod</p>

                                    <p class="text-muted">Although the first prototype was partially validated through testing, a 
                                        device made of silicone rubber only addresses customer requirement 02 regarding reliability to 
                                        an extent. Gastric fluid is highly acidic and contains parietal cells that secrete hydrochloric 
                                        acid (HCI) to inactivate microorganisms (Heda et. al. 2021). Because of this, most gastrostomy 
                                        skin level devices made from medical grade silicone rubber must be removed approximately every 
                                        4 months to ensure the internal bolster does not degrade. However, to achieve long-term 
                                        durability, a material like polytetrafluoroethylene (PTFE) can be used as it is highly resistant 
                                        to HCI between concentrations of 0%-37% and is biocompatible.
                                    </p>

                                    <p class="text-muted">The first change made to the second prototype was reducing the thickness and width of the 
                                        panels that make up the internal bolster to reduce overall material so the panels would nicely 
                                        collapse when stretched. A schematic of the second prototype with labels is shown in Figure 5.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/6.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Labled Schematic of Second Prototype with labels (left) and 
                                        dimensions, in  mm (right)</p>

                                    <p class="text-muted">Other changes made to the design before building the second prototype addressed 
                                        customer requirements 01, safety, and 05, ability for the device to be low-profile.  The external 
                                        bolster was reduced to a diameter of 12.5 mm from 52.5 mm previously to reduce the overall 
                                        volume making the device more low-profile. The tether was increased in width to 2 mm as opposed 
                                        to 1 mm previously to reduce the risk of fracture.
                                    </p>

                                    <p class="text-muted">The cap was updated to feature custom threads to ensure they cannot interface with other 
                                        small-bore connectors that may be in a health care setting. This is a preventative step to 
                                        ensure patient safety and is outlined in ISO 80369-3: Small-bore connectors for liquids and 
                                        gases in healthcare applications — Part 3: Connectors for enteral applications, a series of 
                                        standards developed by the International Organization for Standardization to improve patient 
                                        safety with respect to small-bore connectors in healthcare settings.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/8.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Schematic of Second Prototype Cap with labels (left) and 
                                        dimensions, in mm (right)</p>
                                    
                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">After designing the new model in Solidworks, it was crucial to investigate 
                                        areas of stress concentration for this device as the internal bolster is subject to unique 
                                        force distributions in order to stretch out to fit through the stoma.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/10.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/11.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: FEA Simulation and Mechanical Properties of Internal 
                                        Bolster to Identify Stress Concentration Areas</p>
                                    
                                    <p class="text-muted">These simulations were used to find stress concentration areas only and 
                                        were not used to validate specific stress values
                                    </p>

                                    <p class="text-muted">To account for the stress concentration areas at the corners in the internal 
                                        bolster, fillets were added to the CAD model to distribute the stress. The second prototype was 
                                        3D printed from Formlabs Flexible 80A. Images of the second prototype as well as testing the 
                                        internal bolster are shown in Figure 8.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/12.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Testing of 3D printed prototype for insertion ability 
                                        with thin metal rod</p>

                                    <h6>Risk Analysis:</h6>
                                    <p class="text-muted">Lastly, an FMEA risk analysis table was developed to assess risks in the 
                                        proposed solution and serve as a tool to further update iterated designs.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/13.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: Failure Modes and Effects Analysis (FMEA) for Minimum 
                                        Viable Product (MVP)</p>

                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">Initial testing of my design showed that the prototype could address several of the customer 
                                        requirements I outlined at the beginning of the design process. However, by subjecting the prototype 
                                        to more rigorous testing conditions, like an environment that simulates gastric fluid, I could further 
                                        address potential challenges and refine the design. Fostering innovation within the medical device 
                                        industry allows technology to continually advance and meet the diverse and evolving medical needs of 
                                        all patients.
                                    </p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Algae Filtration for Kelp Growth</h3>
                                        <p class="item-intro text-muted">Abalone-inspired filtration system for addressing issue of diminishing kelp 
                                            forests off the Pacific coast of the U.S. and Mexico</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">
                                        Kelp forests are currently diminishing off the western coast of the United States and Mexico due to 
                                        warming waters and overfishing of white abalone. Kelp forests reduce wave energy therefore decreasing 
                                        the effects of coastal erosion which can wash away homes and the natural coastline. They are also home 
                                        to large populations of sea lions, starfish, and white abalone. The white abalone diet consists of algae, 
                                        which abalone clean from rock surfaces on the ocean floor providing an optimal place for kelp to grow. 
                                        Overfishing of white abalone therefore leads to algae overgrowth and threatens the health of kelp forests.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: (from left to right) Global kelp forest distribution, the 
                                        decrease in living shorelines contributes to coastal erosion and leaves coastal properties more 
                                        vulnurable to natural disasters, kelp bed recovery efforts include aquaculture of giant kelp
                                    </p>
                                    
                                    <h5>Objectives:</h5>
                                    <p class="text-muted">To address the overgrowth of algae due to the decreasing population of 
                                        white abalone, my team and I designed and built a prototype algae filtration system that mimics 
                                        the abilities of white abalone to clean rock surfaces of algae. The system would work in conjunction 
                                        with current efforts to reduce overfishing of white abalone, seed white abalone in restored kelp forests, 
                                        and require preventative action against climate change.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/2.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Full CAD Assembly of System</p>

                                    <h5>Design:</h5>
                                    <h6>Overview</h6>
                                    <p class="text-muted">Our algae filtration system is designed to be secured to 
                                        the rock surfaces in endangered kelp forests via a strong suction cup. It will then be able 
                                        to clean the rock surfaces of algae using a motor-powered brush located on the bottom of the 
                                        system. After the brush releases the algae from the rock surface, a motor-powered turbine will 
                                        produce a vacuum that will pull the now algae-filled water through the filter. Algae will 
                                        become trapped on the underside of the filter while clean water will exit through a top vent 
                                        in the external casing. I created all of the following CAD models and assemblies using 
                                        Solidworks.</p> 

                                    <h6>External Casing</h6>
                                    <p class="text-muted">The external casing is an 11 inch by 7 inch by 4.25 inch enclosure made of 
                                        Nylon 6/6 as this material has high corrosion resistance and behaves well in marine environments 
                                        for extended periods of time. This material is also easily injection moldable meaning it would 
                                        be relatively cheap to manufacture several hundred of these enclosures. The round brush and 
                                        heavy-duty suction cup can be seen in Fig. 2 as well as the openings in the bottom of the external 
                                        casing that lead to the filter. </p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/3.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Brush and suction cup of system</p>

                                    <h6>Internal Components</h6>
                                    <p class="text-muted">The external casing houses two 12 volt (V) DC motors, a turbine (Fig. 3.5), 
                                        two simple filters made of mesh, and a 12V 8 ampere-hour (A hr.) SLA battery, shown as the 
                                        large black box in Fig. 3. The turbine is press-fit onto the output shaft of one of the DC 
                                        motors while the other DC motor is connected to the round brush.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/4.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Interior components of system</p>

                                    <h6>Prototype:</h6>
                                    <p class="text-muted">The system prototype is a 3D printed 1:2 scaled version of the full system 
                                        and includes two DC motors, one suction cup, a 2 inch diameter round brush, and a turbine. The 
                                        battery does not fit inside the scaled enclosure so the DC motors were attached to a 12V power 
                                        adapter for the final prototype testing presentation. The DC motors are waterproofed with 
                                        Sugru™ and marine grease. Images of the prototype are shown below in Figs. 4-6.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/5.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: 1:2 Scaled prototype</p>

                                    <h6>Challenges:</h6>
                                    <p class="text-muted">I thought it would be most meaningful to our presentation to test the 
                                        prototype underwater but this meant the DC motors had to be waterproofed. I originally 
                                        thought sticking them in a food storage container would be adequate but containers I found 
                                        that fit the motors were very large and would require me to scale the enclosure to fit over 
                                        them. I then read this article from robotshop.com about waterproofing motors for marine use 
                                        using Sugru™ and marine grease.</p> 

                                    <p class="text-muted">I rolled out small cylinders of Sugru™ and pressed them into all gaps in 
                                        the external casing and then added marine grease to the output shaft, as 
                                        shown in Fig. 5, to ensure water does not enter between the casing and the shaft.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Waterproofing motor</p>   

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">The prototype was tested in a clear container filled with water. The 
                                        wires from each motor were clamped to the side of the container to ensure they do not come 
                                        in contact with the water. Each motor was connected to a 12V power adapter to test whether the 
                                        motors were fully functional after waterproofing.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: System running underwater</p>

                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">In conclusion, our prototype algae filtration system, designed to combat algae 
                                        overgrowth resulting from the declining white abalone population, has shown promising initial success. 
                                        However, we recognize the need for more comprehensive testing in an environment more similar to the 
                                        conditions of a real kelp forest to ensure its reliability. While the current results are 
                                        encouraging, our focus remains on refining the system through further testing to ultimately 
                                        develop a  self-sustaining system that fosters the growth of resilient and thriving kelp forests.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Our poster for the project which includes a root cause analysis 
                                        of the problem, system requirements for the full system design, images and a description of our prototype, 
                                        and a technical drawing of the assembly
                                    </p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Globe Night Light</h3>
                                        <p class="item-intro text-muted">Design of a small, night light desk ornament along with a 
                                            full manufacturing and assembly process for 500 units</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/1.JPG" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">This project presents my designs as part of Rensselaer 
                                        Polytechnic Institute's (RPI) Manufacturing Processes & Systems (MPS) Laboratory course 
                                        focused on exposing students to common manufacturing techniques used in industry (e.g. plastic 
                                        injection molding, CNC machining, metalforming, and automation).</p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">Each MPS team is tasked with designing a product and writing a technical 
                                        data package (TDP) outlining every step of the manufacturing process with the intention that 
                                        300 of the product will be produced from raw materials in the RPI Manufacturing Innovation 
                                        Learning Lab (MILL). The TDP includes all information for the product including product 
                                        component descriptions, a bill of materials, all technical drawings for product components and 
                                        all assembly fixtures, vises, and molds used in the manufacturing process, and detailed 
                                        manufacturing forms including simulations run on each part using software like Mastercam and 
                                        Autodesk Moldflow. The TDP also includes a detailed description of the assembly process 
                                        including part transfer and quality control, and a brief description of how all 500 of the 
                                        product will be packaged.</p>

                                    <h5>Design:</h5>
                                    <h6>Overview</h6>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Exploded View of Final Design with Labels</p> 

                                    <h6>Baseplate</h6>
                                    <p class="text-muted">The base plate adds weight to the bottom of the assembly such that the user can spin 
                                        the globe without it tipping over. I designed the baseplate with a 3 inch diameter and 0.25 inch height. 
                                        It also features a clearance hole for the 6-32 threaded rod and a 0.05 inch chamfer.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Base plate of product designed for CNC machining from Aluminum 6061</p> 

                                    <h6>Base Body</h6>
                                    <p class="text-muted">The base body is the housing for the 9V battery and power switch. I designed it with 
                                        constant wall thickness of 0.07 inches and a 3 degree draft on all vertical surfaces to abide by 
                                        plastic injection molding design parameters.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Base body designed for plastic injection molding from 
                                        ABS plastic</p>

                                    <p class="text-muted">The base body also has 4 bosses extruded from its underside such that the 
                                        base connector disk can be screwed into the bottom of the base body.</p> 

                                    <p class="text-muted">The “Rensselaer” text is CNC machined into the part post-plastic injection 
                                        molding to avoid use of a side-action cam in the PIM mold.</p> 

                                    <h6>Base Connector</h6>
                                    <p class="text-muted">The base connector interfaces with the bottom surface of the base body and 
                                        is secured by four 4-40 flat head screws. The connector also has a thickness of 0.07 inches 
                                        such that the mold for it can be implemented into the same mold as the base body. A center 
                                        boss with a tapped hole secures the base connector to the threaded rod which extends the 
                                        entire height of the assembly.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Base connector designed for plastic injection molding</p>

                                    <h6>Globe Connector Disk</h6>
                                    <p class="text-muted">The globe connector disk is the interface between both vacuum-formed 
                                        globe halves upon which they are heat staked. The two outermost pins exist for the purpose 
                                        of heat staking. Similar to the base body and base connector, the globe connector has a 
                                        thickness of 0.07 inches such that it can be produced using the same mold as the other two 
                                        parts.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Globe connector designed for plastic injection molding</p>

                                    <p class="text-muted">An off-centered boss with a 2-56 tapped hole was added to secure the 
                                        circular PCBa as shown in Fig. 6. Small pins were also added to act as support pins 
                                        for the PCBa and lift it off the main surface of the globe connector.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Globe connector supporting circular PCBa</p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Electrode Test Setup</h3>
                                        <p class="item-intro text-muted">Setup for positioning 3D-printed neural implant probes 
                                            for stimulation testing</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/1.jpg" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">I designed and built this testing setup for the Panat Lab at Carnegie Mellon 
                                        University. In a collaborative effort, the lab is developing a microelectrode array, the 
                                        <a href="https://www.science.org/doi/full/10.1126/sciadv.abj4853">CMU Array</a>, fabricated from 
                                        depositing metal nanoparticles onto a substrate. The long, narrow shanks are then sintered to create 
                                        conductive paths for bioelectric signals as shown in Fig. 1.
                                    </p>

                                    <div class="row text-center">
                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/4.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 1: Microelectrode array fabricated from 
                                                depositing metal nanoparticles onto a substrate</p>
                                        </div>

                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/5.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 2: Array can be tested in vivo using neural activity 
                                                from anesthetized mice but removing unnecessary animal testing is ideal</p> 
                                        </div>

                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/6.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 3: Benchtop testing allows for neural recording sessions 
                                                using a stimulating electrode to simulate neural activity</p>
                                        </div>
                                    </div>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">The array can be tested in vivo using neural activity (Fig. 2) from anesthetized 
                                        mice but benchtop testing allows for neural recording sessions using a stimulating electrode 
                                        to simulate neural activity (Fig. 3) which can reduce variability between experiments and eliminates 
                                        the need for live animals. Before designing the setup, I outlined 3 main objectives:</p>

                                    <div class="row text-left">
                                        <div class="col-md-4">
                                            <h6>1.</h6>
                                            <p class="text-muted">Reduce movement of the electrode and substrate from outside disruption 
                                                    by securing them in fixtures designed for their unique geometry.</p>
                                        </div>

                                        <div class="col-md-4">
                                            <h6>2.</h6>
                                            <p class="text-muted">Increase accuracy when moving electrode and substrate by 
                                                securing their fixtures to translational stages.</p>
                                        </div>
                                        
                                        <div class="col-md-4">
                                            <h6>3.</h6>
                                            <p class="text-muted">Reduce difficulty of clearing the optical table by securing all hardware 
                                                to an optical breadboard that is screwed into the table and can easily be switched out 
                                                when needed.</p>
                                        </div>
                                    </div>

                                    <h5>Design:</h5>
                                    <p class="text-muted">To address the first objective, I designed a subassembly for the probe. Two 
                                        small translational stages (Thorlabs DT12) are used: one is mounted to the elevated stage and 
                                        allows movement of the arm in the x, and one is mounted to the arm and allows movement of 
                                        clamped substrate in the z.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Probe Subassembly</p>

                                    <p class="text-muted">The substrate clamp restricts all movement of the substrate and ensures the 
                                        electrodes are in the same location for every test. The bottom part of the clamp is permanantly 
                                        secured to the translational stage while the top part can easily be removed via two captive 
                                        screws.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Substrate Clamp</p>


                                    <p class="text-muted">The electrode needed to intercept with the probes at a certain angle and 
                                        was therefore mounted onto a 3D printed part and clamped into place in a similar way to the 
                                        substrate: via a captive screw. I press fit an insert into the bottom part of the electrode 
                                        clamp to ensure the threads wouldn't strip after many uses. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Electrode clamp shown empty with slot channel for electrode 
                                        wire (left), placing the wire into the slot (middle), and a fully clamped electrode (right) 
                                    </p>

                                    <p class="text-muted">The electrode clamp extends from the front camera via a strut channel as 
                                        shown in Fig. 7. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Full assembly with side camera, front camera with electrode
                                        mount, and probe subassembly 
                                    </p>

                                    <p class="text-muted">To mount the cameras, I first designed a 90 degree bracket to secure the 
                                        3-axis translational stages (Thorlabs DT12XYZ) to the aluminum breadboard. Then, I designed 
                                        another bracket to mount the round, lens mounting clamp (Infinity Large Mounting Clamp) to 
                                        the stages. This would allow each camera three degrees of freedom and a secure and minimally
                                        invasive way to mount the cameras. 
                                    </p>

                                    <p class="text-muted">All four of the brackets were first cut from a 1 ft. Aluminum 6061 L bar
                                        and manually machined to obtain the cutout for the stages and necessary holes and 
                                        countersinks as shown in Fig. 8.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Cutting L Bar and Machining Camera Mounts  
                                    </p>

                                    <p class="text-muted">The brackets are shown assembled onto the aluminum breadboard in Fig. 9.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/13.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: Stage bracket (left) supports entire camera assembly 
                                        including XYZ stages, camera, and camera mount, front camera bracket (middle) supports camera
                                        mount and strut channel, and side camera bracket (right) supports camera and camera mount  
                                    </p>

                                    <p class="text-muted">I 3D printed all components of the probe subassembly, press-fit inserts 
                                        into the substrate clamp (Fig. 11) and assembled them onto the laser-cut acrylic stage 
                                        elevated by hex standoffs. The clear container for PBS is a simple storage container from 
                                        The Container Store. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 10: Assembled Probe Subassembly</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/15.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 11: Assembled Substrate Clamp</p>
                                    
                                    <h5>Evaluation:</h5>
                                    
                                    <p class="text-muted">Camera positioning is crucial for the test as the XYZ stage also moves the 
                                        electrode wire for stimulating each probe. Example photos I took to test camera positioning
                                        are shown in Fig. 12.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/16.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 12: Positioning Cameras</p>
                                    
                                    <p class="text-muted">Unfortunately, my internship in the lab ended before I could conduct further testing, 
                                        but potential future evaluations could involve rigorous testing of long-term stability, real-time 
                                        data processing capabilities, and correlation studies with in vivo experiments.</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">
                                        The microelectrode array testing setup I designed and built allows for benchtop testing with a stimulating 
                                        electrode, reducing variability and eliminating the need for live animals.  
                                    </p>
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Redwood Desk Organizer</h3>
                                        <p class="item-intro text-muted">Adjustable length desk organizer made of repurposed redwood</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/1.jpg" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">At one of my internships I sat at a desk right next to an aisleway which sometimes
                                        got very busy and distracting. I had this idea that I wanted some kind of separator that also doubled 
                                        as an organizer for some of the tools I used everyday. None of the desk organizers I could find on 
                                        Amazon were quite what I wanted so when I got home I quickly modeled my idea.</p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">I wanted the design to have an elevated surface for my plant to sit on. 
                                        This, along with compartments to organize my desk items, would act as a separator 
                                        between my desk and the aisleway next to my desk. I also wanted the freedom to adjust the size of 
                                        the organizer in case I ended up needing more space on the desktop.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Initial CAD model</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/3.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Visualization of two sliding compartments</p>
                                    
                                    <h5>Approach:</h5>
                                    <p class="text-muted">I used repurposed redwood from an old play structure in my backyard that my dad 
                                        had recently dissasembled. We first rip cut large pieces of the wood such that they were the correct width
                                        and depth for the boards I needed. I then used the table saw to cut grooves in the vertical standing 
                                        boards for a through dado joint. These joints would better support the shelving than a butt joint.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Joint used to support shelving</p>
                                    
                                    <p class="text-muted">I then assempled all the pieces applying a generous amount of wood glue between each,
                                        and clamped them together. After about 48 hours, I sanded the entire strucutre including any excess wood glue 
                                        from the joints and applied a polyurethane finish to protect the wood from water that might leak from the plant.</p>    
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/5.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Applying a wipe-on polyurethane finish to protect the wood from water</p>
                                    
                                    <p class="text-muted">I am overall very satisfied with how it turned out and used it for the rest of my internship.
                                    </p>    
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: The completed organizer on my desk where I use it to store tools I used daily 
                                        and display my very quickly growing plant
                                    </p>
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>

    </body>
</html>
