<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Charlotte Avra</title>
        
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon_2.ico" />
        
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    
    </head>
    
    
    <body id="page-top">
        
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top" style="background-color: #dfe6da;" id="mainNav">
            <div class="container">
                <a class="navbar-brand" href="#">Charlotte Avra</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars ms-1"></i>
                </button>
                
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#portfolio">Projects</a></li>
                        <li class="nav-item"><a class="nav-link" href="#resume">Resume</a></li>
                        <li class="nav-item"><a class="nav-link" href="#about">About</a></li>
                    </ul>
                </div>

            </div>
        </nav>
        


        <!-- Project Grid-->
        <section class="page-section" id="portfolio">
            <div class="container">
                <!-- <div class="text-center">
                    <h2 class="section-heading text-uppercase">Projects</h2>
                </div> -->
                <br>

                <div class="row">
                    
                    <!-- Project 1-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal4">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/sculptbot/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robotic Clay Sculpting (Research)</div>
                                <div class="portfolio-caption-subheading text-muted">Pre-trained models for deformable object manipulation</div>
                            </div>
                        </div>
                    </div>
                        
                    <!-- Project 2-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal10">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/implant/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Neural Implant Imaging Station</div>
                                <div class="portfolio-caption-subheading text-muted">Automated imaging setup for documentation of micron-scale implant threads</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 3-->
                    <div class="col-lg-4 col-sm-6 mb-4 mb-sm-0">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal5">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/electrode/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">CMU Array Benchtop Testing Setup</div>
                                <div class="portfolio-caption-subheading text-muted">Setup for positioning 3D-printed neural implant probes for stimulation testing</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 4-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal11">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/wafer/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Diced Wafer Cleaning Fixture</div>
                                <div class="portfolio-caption-subheading text-muted">Fixture designed for improved solvent flow and efficiency during chemical
                                    baths, megasonic cleaning, and vapor drying</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 5-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal1">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/gastrostomy/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Gastrostomy Skin Level Device</div>
                                <div class="portfolio-caption-subheading text-muted">Design for medical device for patients who 
                                    require enteral feeding focused on long-term durability and patient comfort</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 6-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal3">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/globe/production.JPG" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Globe Night Light</div>
                                <div class="portfolio-caption-subheading text-muted">Design of a small, night light toy along with a full manufacturing and assembly process for 500 units</div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Project 7-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal9">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/buggy/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Linear Control and Path Planning of Autonomous Vehicle</div>
                                <div class="portfolio-caption-subheading text-muted">Controllers for lateral and
                                    longitudinal control of a Tesla Model 3 in Webots</div>
                            </div>
                        </div>
                    </div>
                    
                    
                    <!-- Project 8-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal2">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/algae/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Algae Filtration for Kelp Growth</div>
                                <div class="portfolio-caption-subheading text-muted">Abalone-inspired filtration system for addressing issue of diminishing kelp forests off the Pacific coast of the U.S. and Mexico</div>
                            </div>
                        </div>
                    </div>
                    

                    <!-- Project 9-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal7">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/mouthrobot/1.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robot Mouth Audio Classification</div>
                                <div class="portfolio-caption-subheading text-muted">Learning human-like tonal inflections 
                                    for studying lip synchronization on a humanoid robot mouth</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 10-->
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal8">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/bookshelf/5.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Robot Librarian</div>
                                <div class="portfolio-caption-subheading text-muted">Nonprehensile manipulation for a shelf organization task</div>
                            </div>
                        </div>
                    </div>

                    <!-- Project 11-->
                    <div class="col-lg-4 col-sm-6">
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal6">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/img/portfolio/redwood/1.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Redwood Desk Organizer</div>
                                <div class="portfolio-caption-subheading text-muted">Adjustable length desk organizer made of repurposed redwood</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        
        <!-- Resume Timeline -->
        <section class="page-section bg-light" id="resume">
            <div class="container">
                <!-- <div class="text-center">
                    <h2 class="section-heading text-uppercase">Resume</h2>
                </div> -->  
                
                <div class="text-center">
                    <h1>Experience</h1>
                </div>

                <ul class="timeline">
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/medra_logo.jfif" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Robotics Engineer</h5>
                                <h6 class="subheading">Medra | June 2024 - Dec. 2025</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="callout-blurb">Unlike traditional lab automation, Medra aims to build a generalizable robotic system that can 
                                    execute end-to-end scientific protocols and automate repetitive lab tasks to accelerate scientific discovery.</p>
                                <p class="text-muted">I owned end-to-end delivery of customer projects, 
                                    from concept and prototyping to development and field deployment. I programmed and tested robotic behaviors to automate
                                    lab workflows, including task optimization, error handling, and motion planning for careful handling. I traveled to customer 
                                    sites and collaborated closely with partner scientists to translate scientific goals into reliable robotic behaviors and 
                                    validate them in their lab environments. 
                                </p> 
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/cmu_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Graduate Researcher</h5>
                                <h6 class="subheading">Mechanical and AI Lab | Aug. 2022 - May 2024</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="callout-blurb">The MAIL lab at CMU combines traditional mechanical engineering with the flexibility of AI to solve 
                                    complex problems, with work ranging from molecular simulations to robotic manipulation.
                                </p>
                                <p class="text-muted">With a team of other researchers, I studied learning-based approaches for robotic manipulation of soft 
                                    materials (e.g. clay sculpting). We leveraged pre-trained models to predict the 
                                    dynamics of materials and plan trajectories. For 3D representation, I used conventional computer vision techniques to stitch 
                                    and segment 3D point clouds for comprehensive scene representation.  
                                </p> 
                            </div>
                        </div>
                    </li>
                    <li> 
                        <div class="timeline-dot"><img class="rounded-circle img-fluid"></div>
                        <div class="timeline-panel">    
                            <div class="timeline-heading">
                                    <h5>Teaching Assistant</h5>
                                    <h6 class="subheading">Engineering Computation | Aug. 2023 - Dec. 2023</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">
                                    I supported students by grading assignments and conducting office hours for 24-780 Engineering Computation, an 
                                    introductory C++ programming course that delves into fundamental data structures and algorithms emphasizing 
                                    the background algorithms used in modern Computer-Aided Design and Computer-Aided Manufacturing tools. 
                                </p> 
                            </div>                                                        
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/medra_logo.jfif" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Robotics Engineer, Intern</h5>
                                <h6 class="subheading">Medra | May - August 2023</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">I wrote and tested robotic protocols for handling biology tools designed for humans including collision avoidance 
                                    in a complex environment, used common computer vision techniques for screen reading and manipulation, and tested and debugged in 
                                    simulation (PyBullet) as well as on a physical 6 DOF manipulator.
                                </p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/nlk_logo.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Mechanical Design Engineer, Intern</h5>
                                <h6 class="subheading">Neuralink | Jan. - June 2022</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="callout-blurb">Neuralink is building a brain-computer interface that translates neural signals into actions, restoring autonomy to 
                                    those with unmet medical needs.</p>
                                <p class="text-muted">I modeled and built a neural implant imaging station and developed an image processing program for documenting the quality 
                                    of the neural implant’s micron-scale threads. My design automated four manual steps from the end-of-line process saving time and risk of damaging 
                                    the implant. I also made improvements to the silicon wafer production process by designing and testing a fixture for processing diced wafers. 
                                    My design improved solvent flow in chemical baths, megasonic cleaning, and vapor drying processes.
                                </p>                          
                        </div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/rpi_logo_2.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Undergraduate Researcher</h5>
                                <h6 class="subheading">Rensselaer Polytechnic Institute | Aug. - Dec. 2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="callout-blurb">The Mills Lab at RPI is an experimental cell and tissue biomechanics laboratory focused on understanding the role of 
                                    mechanics in disease initiation and progression.</p>
                                <p class="text-muted">My project in the Mills Lab at RPI focused on improving the robustness of the existing Traction Force Microscopy (TFM) 
                                    image processing program to more accurately measure cellular traction forces using images of Schwann cells. Estimating these traction forces 
                                    can lead to information about the development of large, soft tumors known as plexiform neurofibromas seen in patients with Neurofibromatosis Type 1 (NF1).
                                </p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/nlk_logo.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Mechanical Design Engineer, Intern</h5>
                                <h6 class="subheading">Neuralink | May - Aug. 2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class = "text-muted">Modeled and built fixture for end-of-line electrode impedance testing implemented 
                                    on neural implant R&D production line, designed parts for machining and injection molding and drafted technical drawings</p>
                            </div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-dot"><img class="rounded-circle img-fluid"></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Manufacturing Technician, Intern</h5>
                                <h6 class="subheading">Neuralink | Jan. - May 2021</h6>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Built neural implant test devices and designed 
                                solutions to improve the production process which included stress testing, soldering, 
                                die bonding, thermal sealing, and leak testing.</p></div>
                        </div>
                    </li>
                    <!--
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/crate_barrel_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Stock Associate</h5>
                                <h6 class="subheading">Crate and Barrel | May - Aug. 2019</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Recieved and tracked orders and shipments, prepared online orders for shipping and pickup 
                                </p>
                            </div>
                        </div>
                    </li>
                    -->
                </ul>
                
                <div class="text-center">
                    <h1>Education</h1>
                </div>

                <ul class="timeline">
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/cmu_logo.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Master of Science, Mechanical Engineering</h5>
                                <h6 class="subheading">Carnegie Mellon University | 2022-2024</h6>
                            </div>
                            <div class="timeline-body">  
                                <p class="text-muted">Emphasis: Robotics and Controls</p>
                                <p class="text-muted">Achievements: BRIDGE Fellowship (full tuition & stipend, 2022 - 2024), Co-author, SculptBot: 
                                    Pre-Trained Models for 3D Deformable Object Manipulation (presented at IEEE ICRA 2024)</p>
                                <p class="text-muted">Relevant Coursework: Modern Control Theory, Learning for Manipulation, Deep Learning, C++, 
                                    Computer Vision
                                </p>
                            </div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/img/logos/rpi_logo_2.png" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h5>Bachelor of Science, Mechanical Engineering</h5>
                                <h6 class="subheading">Rensselaer Polytechnic Institute | 2018-2021</h6>
                            </div>
                            <div class="timeline-body">
                                <p class="text-muted">Achievements: Inventors’ Studio Innovator Award (2021)</p>
                                <p class="text-muted">Relevant Coursework: Elements of Mechanical Design, Modeling and Control
                                    of Dynamic Systems, Electronic Instrumentation, Finite Element Analysis, Fluid Dynamics  
                                </p>
                            </div>
                        </div>
                    </li>
                </ul>

                <div class="text-center">
                    <a href="resume/avra_charlotte_resume_1205.pdf">
                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                        View as PDF
                        </button>
                    </a>
                </div>
                
                <!-- Resume Skills -->
                    <div class="text-center">
                    <h1>Skills</h1>
                    </div>
                   
                    <div class="row text-left">
                        <div class="col-md-4">
                            <h5 class="my-3">Design:</h5>
                            <p class="text-muted">Solidworks, NX, GD&T, tolerance stack-up, material selection, DFM/DFA,
                                PDM systems
                            </p>
                        </div>

                        <div class="col-md-4">
                            <h5 class="my-3">Robotics:</h5>
                            <p class="text-muted">Python, C++, OpenCV, Git, CMake, depth cameras, optical sensors, motion planning 

                            </p>
                        </div>

                        <div class="col-md-4">
                            <h5 class="my-3">Prototyping and Fabrication:</h5>
                            <p class="text-muted">3D printing, laser cutting, fixture building, motors, actuators, manual machining,
                                injection molding, vacuum forming
                            </p>
                        </div>

                    </div>

                    <div class="row text-left">
                        <div class="col-md-4">
                            <h5 class="my-3">Machine Learning:</h5>
                            <p class="text-muted">PyTorch, Pandas, classification, model training, feature extraction, 
                                    data augmentation, fine-tuning</p>
                        </div>
                    </div>
            </div>
        </section>
        

        <!-- About-->
        <section class="page-section" id="about">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">About</h2>
                </div>
                <div class="row">
                    <div class="col-lg-4">
                        <div class="team-member">
                            <img class="mx-auto rounded-circle" src="assets/img/about/profile.jpg" alt="..." />
                            <p></p>
                            <a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/charlotteavra/" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://github.com/charlotteavra" aria-label="GitHub"><i class="fab fa-github"></i></a>
                            <a class="btn btn-dark btn-social mx-2" href="https://scholar.google.com/citations?user=wz2YZsYAAAAJ&hl=en&authuser=1" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                        </div>
                    </div>
                    <div class="col-lg-8">
                        <p>Hi, thanks for visiting my website! My name is Charlotte and I'm a mechanical engineer with hands-on experience 
                            in robotics, medical devices, and neural-interface technologies. I enjoy problem solving, designing, and making, 
                            especially alongside teams of passionate engineers and researchers. At Carnegie Mellon, I explored robotic manipulation 
                            and learning for deformable object handling, then applied robotics software to automate lab workflows at Medra. Now, I’m 
                            returning to my core focus in mechanical engineering, excited to tackle challenging projects and turn ideas into real, 
                            production-ready hardware. 
                            </p>   
                    </div>
                </div>
            </div>
        </section>
        
        
        <!-- Footer-->
        <footer class="footer py-4">
            <div class="container">
                <div class="row align-items-center">
                    <!--<div class="col-lg-4 text-lg-start">Copyright &copy; Your Website 2022</div>-->
                    <div class="col-lg-4 my-3 my-lg-0">
                        <!--<a class="btn btn-dark btn-social mx-2" href="https://www.linkedin.com/in/charlotteavra/" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a>-->
                        <!--<a class="btn btn-dark btn-social mx-2" href="https://github.com/charlotteavra" aria-label="GitHub"><i class="fab fa-github"></i></a>-->
                        <!-- <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="Twitter"><i class="fab fa-twitter"></i></a>
                        <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="Facebook"><i class="fab fa-facebook-f"></i></a> 
                        <a class="btn btn-dark btn-social mx-2" href="#!" aria-label="LinkedIn"><i class="fab fa-linkedin-in"></i></a> -->
                    </div>
                    <div class="col-lg-4 text-lg-end">
                        <!--<a class="link-dark text-decoration-none me-3" href="#!">Privacy Policy</a>
                        <a class="link-dark text-decoration-none" href="#!">Terms of Use</a>-->
                    </div>
                </div>
            </div>
        </footer>
        


        <!-- Portfolio Modals-->
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robot Librarian</h3>
                                        <p class="item-intro text-muted">Nonprehensile manipulation for a shelf organization task</p>
                                        <p class="item-intro text-muted">
                                            <a href="https://github.com/charlotteavra/robot_librarian">[CODE]</a>
                                        </p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/5.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">
                                        Nonprehensile manipulation refers to manipulation not adapted for or not involving antipodal grasping. 
                                        It therefore leverages a wider variety of primitives, such as pushing, sliding, rolling, and tipping 
                                        often allowing for certain object interactions that would otherwise be difficult or even impossible with 
                                        only a grasping primitive. However, this wider variety of primitives can also lead to challenges in 
                                        robotic sensing and control whereby sensitivities like geometry, mass, and friction become more of an issue.
                                    </p>
                                    <p class="text-muted">
                                        Shelves present a unique challenge in robotics as their confined spaces often limit manipulability and 
                                        graspability of the objects inside. These challenges are present in real-world scenarios, especially in 
                                        dynamic environments like kitchens, warehouses, and retail spaces, that demand versatile interactions.
                                    </p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">
                                        In this project, my team and I investigated a shelf organization task and planned efficient robot-pushing 
                                        motions, enabling the reorganization of boxes leaning on a shelf to create space for an additional box. 
                                        Given the first box is placed vertically and the second box is placed at a random angle, the task is to 
                                        plan a nonprehensile push motion to tilt the second box until it is vertical, allowing for a third box to 
                                        be placed. 
                                    </p>

                                    <h5>Method:</h5>
                                    <p class="text-muted">
                                        We first categorized the possible box configurations within the project scope into 4 cases:
                                    </p>
                                    <ul>
                                        <li>
                                            <p class="text-muted">
                                                Case 1: Indicates there is space to insert a third box with no collisions (boxes 1 and 2 
                                                are treated as obstacles)
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 2: Indicates the third box can be placed by treating boxes 1 and 2 as movable rigid
                                                bodies
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 3: Indicates there is no space in the current configuration for the third box to
                                                be placed. The robot must execute a nonprehensile pushing motion to create space for
                                                the third box.
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Case 4: Indicates all three boxes are in the goal configuration: vertical orientation 
                                                inside the bookshelf.
                                            </p>
                                        </li>
                                    </ul>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Examples of cases 1-4 (from left to right). The blue 
                                        highlighted region indicates a space large enough to place the third box. 
                                    </p>

                                    <p class="text-muted">
                                        Fig. 2 illustrates the general pipeline of our framework. We learned a classifier that determines 
                                        whether of not a box can be successfully placed in a target location given a depth image of the 
                                        environment. The classifier was trained on depth images collected from simulation of the scene 
                                        initialized with different box configurations. The images were automatically labeled by first 
                                        checking if all three boxes were within the boundaries of the shelf at the end of the robot
                                        action, and whether the two initially placed boxes had moved greater than a threshold distance.      
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/flowchart.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: General framework of our approach 
                                    
                                    <h6>Sampling-based Approach:</h6>
                                    <p class="text-muted">
                                        I explored our first approach for planning an efficient robot-pushing motion which involved uniform random 
                                        sampling of contact points on the surface of the box, simulating a push from that location, and evaluating 
                                        the quality of the push. This approach is uninformed, meaning it does not exploit any specific knowledge of 
                                        the goal and only makes decisions based on what is immediately visible in the current state. It therefore 
                                        also does not guarantee a successful push given a certain box configuration. However, it provides a starting 
                                        point for later, more informed approaches.   
                                    </p>

                                    <p class="text-muted">
                                        We capitalize on the assumption that a physics simulator (PyBullet) can be used within the planning loop to 
                                        evaluate pushing actions. I first randomly sampled 200 contact points on the inclined box. An end-effector 
                                        pose was then defined for each contact point using the (x,y,z) coordinates of the contact point and 
                                        constraining the orientation to vertical with respect to the world frame as shown in Fig. 3. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Sampling contact points from surface of angled box and defining end-effector 
                                        pose based on sampled point</p>

                                    <p class="text-muted">
                                        The corresponding joint angles were then computed using <code>p.calculateInverseKinematics()</code>. If the current
                                        joint configuration was in collision or the IK failed, a zero score was assigned to the attempted push. 
                                        Conversely, if the configuration was collision-free, the push was executed using a simple motion primitive defined as 
                                        a straight-line movement along the world frame x-axis, opposing the x normal of the sampled point, effectively 
                                        pushing against the surface of the box. The heuristic used to evaluate the push was a score proportional to the distance 
                                        between the final orientation of the box, <code>p</code>, and the goal vertical orientation, <code>q</code>, both 
                                        represented as quaternions:
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/score.png" alt="..." />

                                    <h6>Search-based Approach:</h6>
                                    <p class="text-muted">
                                        Unfortunately, the previous approach does not exploit any specific knowledge of the goal to make decisions. 
                                        To take a more informed approach, I also attempted a search-based method. Similar to the previous appraoch, 
                                        I use the physics simulator within the planning loop.
                                    </p>

                                    <p class="text-muted">
                                        I first discretized the surface of the box into a grid of points as shown in Fig. 4. I then used an A* 
                                        search to find the optimal path to the goal node, which represents the physical location on the box 
                                        from which to initiate a push. I used a heuristic similar to the scoring function used in the 
                                        sampling-based approach to define a "cost-to-go" estimation such that nodes with lower cost-to-go 
                                        estimations will be explored earlier. <code>p</code> is again the final orientation of the box and 
                                        <code>q</code> is the goal vertical orientation, both represented as quaternions: 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/heuristic.png" alt="..." />

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/grid.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Discretizing box surface into grid of contact points</p>
                                    
                                    <p class="text-muted">
                                        I again used a straight-line push motion primitive when evaluating push actions but I changed the 
                                        orientation of the end-effector to be parallel with the surface of the box instead of vertical with 
                                        respect to the world frame. The discretized points are organized as a <code>numpy.meshgrid</code>. 
                                        In the planning loop, the successors of each node are simply the eight neighbors of the current node.  
                                    </p>

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">
                                        We ran 50 randomly generated trials and recorded the average plannning time and success rate. In each 
                                        trial, the first box is placed vertically, and the second box is placed at a random angle between 
                                        <code>(-pi/4, -pi/6)</code> and <code>(pi/6, pi/4)</code>. We then run 100 steps of the simulation to allow 
                                        the boxes to settle. 
                                    </p>

                                    <h6>Sampling-based Approach:</h6>
                                    <p class="text-muted">
                                        The uninformed sampling approach achieved a 20% success rate across all 50 trials, and an average planning 
                                        time of 25.1 seconds.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/sample_gif_success.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Example of a successful push action (score = 0.998)</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/sample_gif_fail.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Example of a failed push action (score = 0.794)</p>
                                    
                                    <h6>Search-based Approach:</h6>
                                    <p class="text-muted">
                                        The success rate increased to 78% for the search-based approach and the planning time reduced by approximately 
                                        20 seconds, with an average planning time of 5.08 seconds per trial, compared to the sampling-based approach.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/search_gif_2.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Example of a successful push action (h(s) = 0.0042)</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">
                                        Prehensile manipulation can set artificial limits on the range of tasks a robot can perform. Dynamic nonprehensile 
                                        manipulation, which leverages a wider variety of primitives such as pushing, sliding, rolling, and tipping, exploits 
                                        dynamics, thereby allowing for certain object motions that would otherwise be difficult or even impossible with only 
                                        a grasping primitive. We explore nonprehensile manipulation in a confined shelf environment where various tasks, 
                                        such as inserting and organizing are difficult when constrained to prehensile manipulation only. We find that 
                                        our search-based approach performs significantly better than our uninformed sampling approach by increasing success 
                                        rate and decreasing planning time. For future work, a more sophisticated heuristic could be investigated as well 
                                        as a more sophisticated push motion primitive.   
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/bookshelf/7.png" alt="..." />
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal9" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Linear Control and Path Planning of Autonomous Vehicle</h3>
                                        <p class="item-intro text-muted">Controllers for lateral and
                                            longitudinal control of a Tesla Model 3 in Webots</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">There have been significant advancements in autonomous vehicle technology in 
                                        recent years. As more self-driving vehicles are being developed and tested, there is an increasing 
                                        need for robust controllers and efficient path planning algorithms to ensure safe and reliable 
                                        autonomous operation.</p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">The goal of this project was to:</p>
                                    <ul>
                                        <li>
                                            <p class="text-muted">
                                                Develop an optimal controller for lateral and longitudinal control of a simulated Tesla Model 3 in 
                                                Webots making use of the kinematic bicycle model, shown in Fig. 1, for studying the vehicle's dynamics.
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Implement the A* path planning algorithm to re-plan the trajectory given a second car is on the track 
                                                that you must pass. 
                                            </p>
                                        </li>
                                        <li>
                                            <p class="text-muted">
                                                Predict the global position and heading from observable states using an extended Kalman filter (EKF) 
                                                given some localization information is missing.
                                            </p>
                                        </li>
                                    </ul>

                                    <p class="text-muted">
                                        The car drives around a track with the same geometry as the CMU buggy course. Buggy is 
                                        a time-honored CMU tradition where students race human-powered, small, aerodynamic vehicles around Schenley 
                                        Park's Flagstaff Hill. More information on it can be found <a href="https://www.cmu.edu/buggy/">here</a>. Like 
                                        buggy, the goal of the controller is to complete the 0.84 mile course minimizing both time and deviation from 
                                        the course.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Kinematic bicycle model</p>

                                    <h5>Method:</h5>
                                    <h6>Discrete-Time Linear Quadratic Regulator (LQR):</h6>
                                    <p class="text-muted">I first implemented an infinite horizon discrete-time Linear Quadratic Regulator (LQR) controller 
                                        for lateral control of the vehicle and a simple PID control for longitudinal control. The error-based linearized state 
                                        space model for the lateral dynamics is given by:  
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lateral_dynamics.png" alt="..." />

                                    <p class="text-muted">where e<sub>1</sub> is the cross-track error, or distance to the center of gravity of the 
                                        vehicle from the reference trajectory, and e<sub>2</sub> is the heading error, or orientation error of the vehicle 
                                        with respect to the reference trajectory.</p>

                                    <p class="text-muted">The longitudinal dynamics are given by:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/long_dynamics.png" alt="..." />

                                    <p class="text-muted">Given the <code>A</code> and <code>B</code> matrices, I then discretize the lateral state space model 
                                        using <code>signal.cont2discrete(system=(A,B,C,D))</code>, where <code>C</code> is an identity matrix with the same size as 
                                        <code>A</code>, and <code>D</code> is a matrix of zeros with the same size as <code>B</code>. I then tune 
                                        my <code>Q</code> and <code>R</code> matrices before solving the discrete-time algebraic Riccati equation (ARE) using 
                                        <code>linalg.solve_discrete_are(A, B, Q, R)</code>. Lastly, we can define the LQR gain as:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_gain.png" alt="..." />
                                    
                                    <p class="text-muted">and compute the control outputs using:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/input.png" alt="..." />
                                    
                                    <h6>PID Controller for Longitudinal Control:</h6>
                                    <p class="text-muted">For the PID controller, I define a minimum and maximum speed as <code>speed_min</code> and <code>speed_max</code>, 
                                        chosen emperically. I then compute the curvature of the track at the current location using the closest waypoint and a waypoint at a 
                                        certain look ahead distance (also defined emperically), and set the <code>ideal_speed</code> of the car to be 
                                        <code>max(speed_min, min(speed_max, (1/curvature)))</code>. Then, the <code>speed_error</code> is simply <code>(ideal_speed-current_speed)</code>, 
                                        the integrated speed error is <code>total_speed_error</code>, and the derivative of the speed error is <code>speed_error_rate</code>. Lastly, 
                                        I tune my PID gains <code>Kp</code>, <code>Ki</code>, and <code>Kd</code>, and compute the output throttle: 
                                        <code>F = (Kp*speed_error) + (Ki*total_speed_error) + (Kd * speed_error_rate)</code>.
                                    </p>
                                    
                                    <h6>A* Path Planning for Obstacle Avoidance:</h6>
                                    <p class="text-muted">Obstacle avoidance is crucial in autonomous vehicle control. In the previous example, there was only 
                                        one vehicle on the track, but if another is introduced, we must determine how to safely overtake it. For this, I take a 
                                        very simplified approach where I assume we will overtake the other vehicle in the straight-away so we can re-plan the 
                                        trajectory once and follow the new trajectory open-loop. </p>

                                    <p
                                     class="text-muted">To re-plan the trajectory, we can first compute the H, G, and F values of the known start node, and 
                                        add it to the open heap queue using <code>heappush(open_list, start_node)</code>. I then implement a loop to continue 
                                        searching the map until the shortest path to the end node is reached. Starting at 7:35 in 
                                        <a href="https://youtu.be/-L-WgKMFuhE?si=lchF4JSsQKhJp_MY&t=455">this video</a>, there is a great explanation of this 
                                        search.
                                    </p>
                                    
                                    <h6>Extended Kalman Filter Simultaneous Localization and Mapping (EKF SLAM)</h6>
                                    <p class="text-muted">Finally, in the previous implementations, we are given the global coordinates of the 
                                        course trajectory, but these are not always available in real-world scenarios. Localization information from GPS 
                                        could be missing or inaccurate in tunnels or in close proximity to tall infrastructure. In this case, we do not 
                                        have direct access to the global position, <code>X</code> and <code>Y</code>, and heading, <code>psi</code>, and 
                                        must estimate them from observable states in the vehicle
                                        frame and range and bearing measurements of map features. I used an extended Kalman filter (EKF) for predicting the 
                                        distances to map coordinates whose global coordinates are provided.
                                    </p>

                                    <p class="text-muted">Let the state vector be:</p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/state_vector.png" alt="..." />
                                    <p class="text-muted">where there are <code>n</code> map features at global position <code>m</code>.</p>

                                    <p class="text-muted">
                                        The ground truth of these map feature positions are static but unknown, meaning they will not move but 
                                        we do not know where they are exactly. However, the vehicle has both range and bearing measurements 
                                        relative to these features:
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/measurement_system.png" alt="..." />

                                    <p class="text-muted">
                                        We can assume a closed-form expression for the predicted state as a function of the previous state, and 
                                        the measurement can be a function of the state and the measurement noise (more info 
                                        <a href="https://www.mathworks.com/help/driving/ug/extended-kalman-filters.html">here</a>):
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/x_y.png" alt="..." />

                                    <p class="text-muted">
                                        Then, I computed <code>F</code>, the Jacobian of the predicted state with respect to the previous state, 
                                        and <code>H</code>, the Jacobian of the measurement with respect to the state. I computed these matrices 
                                        by hand.
                                    </p>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/F_H.png" alt="..." />

                                    <p class="text-muted">
                                        With <code>n = 8</code> map features, <code>F</code> is a <code>(3 + 2n, 3 + 2n) = (19, 19)</code> size 
                                        array, and <code>H</code> is a <code>(2n, 3 + 2n) = (16, 19)</code> size array. 
                                    </p>

                                    <p class="text-muted">
                                        I then iteratively "predicted and corrected", predicting the state and error covariance, then updating both 
                                        estimates after computing the Kalman gain.  
                                    </p>
                                    
                                    <h5>Evaluation:</h5>
                                    <h6>Discrete-Time Linear Quadratic Regulator (LQR):</h6>
                                    <p class="text-muted">The tuned LQR controller completed the track in approximately 120 seconds with a 
                                        maximum deviation of 4.69 meters and an average deviation of 0.41 meters as shown in Fig. 2.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Performance of linear quadratic regulator</p>

                                    <h6>A* Path Planning for Obstacle Avoidance:</h6>
                                    <p class="text-muted">I evaluated the performance of my A* implementation with a few arbitrary costmaps as shown 
                                        in Fig. 3, to ensure it was behaving the way I intended. These maps do not relate to the track but provide 
                                        a more general evalutaion of the function.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/astar_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Performance of A* path planning algorithm on arbitrary costmaps</p>
                                    
                                    <p class="text-muted">Fig. 4 shows the performance on the actual track where the yellow in the plot represents obstacles. 
                                        It can also be seen that we rather naively represent the second car as a large static obstacle. This means 
                                        this approach works only when the second car moves at the same velocity each race, and that once we pass the 
                                        second car in the straightaway, we must stay ahead of it to avoid collision.  
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/astar_track_performance.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/gif_1.gif" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Performance of A* path planning algorithm on a section of the track (top) and 
                                        a GIF of the resulting simulation showing the overtaking of the second vehicle (bottom)
                                    </p>

                                    <h6>Extended Kalman Filter Simultaneous Localization and Mapping (EKF SLAM)</h6>
                                    <p class="text-muted">
                                        I tested the performance of my EKF implementation on an arbitrary trajectory as shown in Fig. 5, before 
                                        integrating it into my Webots simulation. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/ekf_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Performance of EKF implementation on arbitrary trajectory</p>
                                    
                                    <p class="text-muted">The tuned LQR controller with EKF SLAM completed the track in approximately 121 seconds. 
                                        The maximum deviation increased slightly from 4.69 meters when global position was known to 5.10 meters using 
                                        global position predictions and the average deviation from 0.41 meters to 0.84 meters as shown in Fig. 6.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/buggy/lqr_ekf_performance.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Performance of EKF implementation using previously described LQR 
                                        controller on Buggy course in Webots</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">In conclusion, studying controllers and path planning for autonomous vehicles is essential 
                                        to address the evolving challenges and opportunities in the field and to pave the way for the safe and 
                                        widespread adoption of autonomous vehicle technology.</p>
                                    
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robotic Clay Sculpting (Research)</h3>
                                        <p class="item-intro text-muted">Behavioral Cloning and Dynamics Prediction for Robotic Clay Manipulation</p>
                                        <p class="item-intro text-muted">
                                            <a href="https://arxiv.org/abs/2309.08728">[PAPER]</a> | 
                                            <a href="https://sites.google.com/andrew.cmu.edu/sculptbot/home">[WEBSITE]</a>
                                        </p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/1.jpg" alt="..." />

                                    
                                    <h5>Abstract:</h5>
                                    <p class="text-muted">
                                        Deformable object manipulation presents a unique set of challenges in robotic manipulation including high degrees of freedom, complex dynamics, and severe 
                                        self-occlusion. We investigate these challenges through two approaches: learning a latent dynamics model with point clouds as the state representation for 
                                        predicting material deformations given a grasp action and training a transformer-based behavioral cloning (BC) framework for predicting action sequences. 
                                        Both approaches are focused on the task of robotic clay sculpting with a parallel gripper and all data collection and experiments are conducted on a physical 
                                        robot. Our results demonstrate the effectiveness of the latent dynamics model to capture the dynamics of the clay and, when combined with model predictive 
                                        control (MPC) and our action sampler, generate a range of simple shapes. However, we find the transformer-based BC framework struggles to learn a comprehensive 
                                        manipulation policy due to challenges with the highly multimodal nature of the sculpting task. Through these methods and corresponding experiments, we demonstrate 
                                        the generalization capability of the latent dynamics model and the value of leveraging 3D state representations. The limitations of our transformer-based BC 
                                        approach reveal the necessity for further research into imitation learning methods for deformable object manipulation that better handle multimodal action 
                                        distributions in the demonstration trajectories.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Robotic manipulation of deformable objects has applications in the automotive industry with the complex assembly of wire 
                                        harnesses, surgical robotics, cooking and service robotics, and the agriculture industry where robots may be handling soft fruits and vegetables.
                                    </p> 
                                    
                                    <h5>Methods:</h5>
                                    <p class="text-muted">We investigate these challenges using the task of robotic clay sculpting and approaching the task with two methods: predicting clay dynamics and 
                                        predicting actions.</p>

                                    <h6>Vision System:</h6>
                                        <p class="text-muted">
                                        Both processes share the same visual data processing pipeline:</p>
                                        <ul>
                                            <li>
                                                <p class="text-muted">RGBD data from workspace cameras are registered for a full point cloud representation of the clay. We calibrate four 
                                                    workspace cameras and use the RANSAC global registration algorithm to roughly align the point clouds. We then use the iterative closest point (ICP) 
                                                    algorithm for refinement.</p>
                                            </li>
                                            <li>
                                                <p class="text-muted">Position and color-based cropping are used to isolate the clay. Due to the top-down images 
                                                    captured from the RGBD cameras, the base of the object is occluded. To form a fully enclosed point cloud shell, 
                                                    we combine the point cloud with a base plane of generated points.</p>       
                                            </li>
                                            <li>
                                                <p class="text-muted"> 
                                                    Point clouds are downsampled to 2048 points using uniform random sampling for faster training.</p>
                                            </li>
                                        </ul>
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Data processing pipeline for point clouds 
                                    </p>

                                    <h6>Latent Dynamics Model:</h6>
                                    <p class="text-muted">We leverage the tokenizer from <a href="https://arxiv.org/abs/2111.14819">Point-BERT</a>, a pre-trained model trained on the ShapeNet dataset, to learn 
                                        a latent representation of the input point cloud data.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Point-BERT learns a quality reconstruction of the input point cloud data evaluated using Chamfer Distance (CD) 
                                    </p>

                                    <p class="text-muted">Within this latent space, we predict how features of each point cloud cluster change and deform and output next-state 
                                        discrete tokens using a <a href="https://www.sciencedirect.com/science/article/pii/S0893608018302636">DGCNN</a> token predictor 
                                        model. The DGCNN predicts how each cluster’s point token changes given a grasp action. The output next-state discrete tokens are then decoded by the Point-BERT 
                                        dVAE decoder to output a predicted, next-state, fully dense point cloud in real space.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Latent dynamics model prediction pipeline 
                                    </p>

                                    <h6>Imitation Learning:</h6>
                                    <p class="text-muted">The transformer-based behavioral cloning (BC) framework predicts a sequence of actions to achieve a goal state. We train two 
                                        <a href="https://tonyzhaozh.github.io/aloha/">action-chunking transformer (ACT)</a> policies: (1) with input point cloud observations embedded using Point-BERT, 
                                        and (2) with top-down image observations embedded using the default ResNet encoders. We apply rotation augmentations to the point clouds increase the number of 
                                        trajectories used to train. Rotation augmentations consist of centering the point clouds on the z-axis and applying a z-rotation with a corresponding z-rotation 
                                        of the robot end-effector. 
                                    </p>
                                    
                                    <h5>Experiments:</h5>
                                    <p class="text-muted">We collect two separate datasets on a physical Franka Emika Panda manipulator: a random
                                        action dataset and human demonstration dataset. The random action dataset is collected by randomly sampling action parameters and executing the 
                                        generated grasps. A point cloud of the clay is recorded before and after each grasp. The human demonstration dataset is collected using kinesthetic 
                                        teaching where the human demonstrator controls the end-effector position, rotation, and the distance between the fingertips. A point cloud of the clay is again 
                                        recorded before and after each grasp. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Data collection hardware setup
                                    </p>

                                    <p class="text-muted">During initial data collection, we were noticing the robot grippers would unintentionally pick up the clay rendering it difficult 
                                        to collect more difficult to collect data in the same trajectory. We designed a small part that would act as an anchor to hold the middle of the 
                                        clay down to the elevated stage.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: CAD model of the data collection setup with the Franka manipulator, elevated stage for the clay (initialized to
                                        a cylinder), and a small 3D printed screw that acted as an anchor to keep the center relatively restrained to the stage
                                    </p>

                                    <h6>Latent Dynamics Model:</h6>
                                    <p class="text-muted">We find that the dynamics model effectively captures the dynamics of the clay and predicts next-state. 
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Next-state dynamics predictions of the clay evaluated using Chamfer Distance (CD)
                                    </p>

                                    <p class="text-muted">We plan action trajectories using model predictive control (MPC) and evaluate the results of different sculpting tasks. The dynamics 
                                        model trained on the human demonstration dataset had a 16.1% lower mean chamfer distance than the dynamics model trained on  the random action dataset.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/11.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Real-world shapes created by human (left) and human demonstration trained dynamics model combined with MPC and 
                                        geometric sampling (right)
                                    </p>

                                    <h6>Imitation Learning:</h6>
                                    <p class="text-muted">The transformer-based BC approach struggles with multimodality in the demonstration trajectories. Both of the trained ACT policies converge 
                                        to repeating the most common action from the demonstrations regardless of state observation. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/sculptbot/12.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Target shape (left) and poor results from ACT policies trained on images and point clouds, as compared with a human 
                                        demonstrator (right)

                                    </p>
                                    
                                    <div class="text-center">
                                    <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                        <i class="fas fa-xmark me-1"></i>
                                        Close Project
                                    </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Robot Mouth Audio Classification</h3>
                                        <p class="item-intro text-muted">Learning human-like tonal inflections 
                                            for studying lip synchronization on a humanoid robot mouth</p>
                                        <p class="item-intro text-muted">
                                            <a href="https://github.com/charlotteavra/mouth_robot">[CODE]</a>
                                        </p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">This project attempts to quantify the human-likeness of sound produced by 
                                        a humanoid robot. Humanoid robots are often designed to interact with humans in various settings, 
                                        such as homes, workplaces, or public spaces. Therefore, human-like sound allows robots to communicate 
                                        with humans in a more natural and intuitive way improving user interaction and social integration. 
                                        The robot we used has no speakers and produces sound from a variable pitch pneumatic
                                        sound generator and resonance tube deformed by a series of servo motors along its length as shown in 
                                        Fig. 1.   
                                    </p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">Our goal was to quantify the human-likeness of audio produced by the robot such 
                                        that we can use this metric to rate each sound and iteratively update the robot's hardware to 
                                        produce a more human-like sound.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Humanoid robot mouth used for this project</p>

                                    <h5>Audio Processing:</h5>
                                    <p class="text-muted">The way in which data is processed before inputting it into any kind of 
                                        machine learning or deep learning model is fundamental. In the domain of audio, there are 
                                        several concepts that aid in data pre-processing.
                                    </p>

                                    <h6>Mel Spectrograms:</h6>
                                    <p class="text-muted">Digital representations of audio signals most often begin as the relationship 
                                        of amplitude and time. However, to extract useful information from these signals, a Fourier 
                                        transform can be applied to decompose a signal into its individual frequencies and their 
                                        amplitudes and therefore convert from the time to frequency domain.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Example audio signal represented by signal amplitude in 
                                        the time domain as well as in the frequency domain after applying a Fourier transform 
                                        (Image Source: <a href="https://insightincmiami.org/data-visualization-using-the-fourier-transform/">insightincmiami.org</a>)</p>
                                        
                                    <p class="text-muted">Most speech and music signals are non-periodic. This means that to represent 
                                        these signals in the frequency domain, a Fast Fourier Transform (FFT) is performed over several 
                                        windowed segments of the signal. What results is called a spectrogram. Spectrograms are 
                                        visualizations or figures of audio that represent the spectrum of frequencies over time for 
                                        an audio recording.
                                    </p>

                                    <p class="text-muted">If frequency is converted from Hertz to the Mel Scale, a representation of 
                                        frequency that mimics the perception of sound by humans and hence why it is used often in 
                                        machine learning, the spectrogram is called a Mel Spectrogram.
                                    </p>    
    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Example Mel Spectrograms of a human (left) and our mouth 
                                        robot producing a high-level tone</p>

                                    <h6>Mel Frequency Cepstral Coefficients:</h6>

                                    <p class="text-muted">The Mel Frequency Cepstrum (MFC) is a discrete cosine transformation (DCT) 
                                        on the log of the magnitude of the Fourier spectrum which is obtained by applying a Fourier 
                                        transform on the time signal. MFCC’s are coefficients that collectively make up an MFC. MFCC's 
                                        visually represent features of the audio remarkably well and therefore can be input into a 
                                        convolutional neural network for classification.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Example Mel Frequency Cepstral Coefficients of a human  
                                        producing a dipping tone (left) and a falling tone (right)</p>

                                    <h5>Method:</h5>
                                    <h6>Data Collection:</h6>
                                    <p class="text-muted">We first collected several thousand audio recordings from the robot. To construct the 
                                        dataset, we first structured each data point as a sequence of 3 varied pitches with a 
                                        repeating open and closed actuation of the mouth. We then populated each sequence with an 
                                        initial guess of the relative pitch values of a high-level, rising, dipping or a falling tone, 
                                        then labeled them as such. We executed these tones on the robot and recorded the audio output.
                                        There was an equal class distribution in these data.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Data collection and automatic labeling process</p>

                                    <p class="text-muted">Data for human audio recordings, which we also experimented with, were 
                                        sourced from the <a href="https://tone.lib.msu.edu/">Tone Perfect database</a>. Tone Perfect 
                                        includes 9,840 audio files representing 410 monosyllabic sounds in Mandarin 
                                        Chinese each recorded from six speakers using four different tones: high-level tone, rising 
                                        tone, dipping tone, and falling tone. We used 4,500 of these audio files. 
                                    </p>

                                    <h6>Data Augmentation:</h6>
                                    <p class="text-muted">The iteration time for updating the robot hardware to test different 
                                        configurations in the hopes of obtaining a more human-like sound would be long. Therefore, 
                                        we chose to augment data from the robot in different ways to speed up iteration cycles.
                                    </p>

                                    <p class="text-muted">1. The data augmentation process included first compressing the audio 
                                        signal from three seconds to one second. 
                                    </p>

                                    <p class="text-muted">2. Due to a shortened wavelength when compressing the audio signal, this 
                                        resulted in an increased pitch. Each audio signal was then pitched down by 1.5 octaves to 
                                        return to its original pitch. This was based on human perception of the original pitch and 
                                        was not quantitatively computed.
                                    </p>
                                       
                                    <p class="text-muted">3. The pitch shifting operation caused the decibel level of each 
                                        signal to be reduced significantly so each signal was increased by 15 decibels.
                                    </p>
                                        
                                    <p class="text-muted">4. The pitch shifting operation caused the decibel level of each signal to 
                                        be reduced significantly so each signal was increased by 15 decibels.
                                    </p> 

                                    <p class="text-muted">5. A mic pop at the beginning of each audio signal was removed and 
                                        A 0.25 second fade in/fade out effect was added to each audio signal to mimic the change 
                                        in volume that may occur when a human opens and closes their mouth.
                                    </p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/7.png" alt="..." />
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/8.png" alt="..." /> 
                                        </div>
                                    </div>

                                    <p class="item-intro text-muted">Fig. 6: Waveplots of example raw robot audio signal and 
                                        augmented audio signal
                                    </p>

                                    <p class="text-muted">60 MFCC’s for each audio signal were then computed using the librosa sound 
                                        processing library, zero padding was applied to ensure square format, and then they were 
                                        input into the model. 
                                    </p>

                                    <h6>Model Architecture:</h6>
                                    <p class="text-muted">The CNN architecture we used was adapted from 
                                        <a href="https://github.com/adhishthite/sound-mnist">sound-mnist</a> and has 3 
                                        convolution layers with relu activation and batch normalization after each layer. Then a 
                                        max pooling layer and dropout followed by 3 fully connected layers the last one having 
                                        softmax activation.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: CNN architecture used for all experiments</p>
                                    
                                    <p class="text-muted">The goal of the model was to learn from MFCC's based on audio produced by humans, 
                                        specifically 4,500 signals from the Tone Perfect database, and be tested on 1,120 
                                        MFCC's based on audio produced by the robot to determine if the robot sounds were human-like. 
                                    </p>

                                    <p class="text-muted">We wanted the model to generalize enough such that it could maintain high 
                                        accuracy given a validation set of MFCC's that may look very different from what it was trained 
                                        on, but still reflect the human-likeness of the validation set through its softmax output 
                                        predictions: high value predictions for human-like sounds and low value predictions for 
                                        non-human-like sounds. This, however, proved to be a difficult goal. 

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">We first show here four examples of the MFCC’s that the model learned and 
                                        was tested on. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/11.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/12.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/13.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: MFCC's visualizing four tonal inflections produced by 
                                        a human (left) and our mouth robot (right)
                                    </p>
                                    
                                    <h6>Training and Testing on Robot MFCC's:</h6>
                                    <p class="text-muted">We first split the 1,120 robot MFCC's with 80% (880 MFCC's) for training and 
                                        20% (220 MFCC's) for validation. When trained on 880 robot audio signals, the validation 
                                        accuracy was 92.4% as shown in Fig. 9. This proves the model can successfully classify robot 
                                        MFCC's when trained on them with high accuracy validating the choice of model architecture 
                                        for the remaining experiments.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/15.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: High model performance when trained and validated using only
                                        robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, Epochs = 30, 
                                        Batchsize = 20)</p>

                                    <h6>Training on Human MFCC's and Testing on Robot MFCC's:</h6>
                                    <p class="text-muted">For this experiment, we used all 4,920 human MFCC's for training and 
                                        validated the model using all 1,120 robot MFCC's. This time, the model had difficulty 
                                        recognizing features in the validation set and yielded a 25.00% accuracy.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/17.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 10: Poor model performance when trained on human MFCC's and 
                                        validated using robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, 
                                        Epochs = 50, Batchsize = 20)</p>

                                    <p class="text-muted">At around 33 epochs, the validation loss became greater than the training 
                                        loss, as shown in Fig. 10, which provides an indication of overfitting in the training data. 
                                        The training accuracy was above 99% which is another indication the model began to overfit 
                                        the training data and is therefore unable to generalize. This resulted in heavy overprediction 
                                        of class 3.
                                    </p>

                                    <p class="text-muted">The model was then retrained on the human audio signals with 30 epochs, as 
                                        shown in Fig. 11, in an attempt to limit overfitting. The model performed slightly better with 
                                        an accuracy of 25.71% and was able to predict tones beyond class 3 but was still largely 
                                        overpredicting class 3.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/mouthrobot/16.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 11: Poor model performance when trained on human MFCC's and 
                                        validated using robot MFCC's (Hyperparameters used: Adam optimizer, Learning rate = 0.0001, 
                                        Epochs = 30, Batchsize = 20)</p>

                                    <h5>Conclusion:</h5>
                                    <h6>Future Work:</h6>
                                    <p class="text-muted">As this was only a semester long project, we weren't able to run all the 
                                        experiments we wanted to. For future work, we would look more into updating the hardware 
                                        by either replacing the bagpipe reed with a sound generation mechanism with a larger span of 
                                        possible pitches or replacing our air pump with one with a larger span of possible speeds. This 
                                        would all be in an attempt to manually produce a sound that sounds more human-like, before 
                                        studying which features a neural network uses to classify each tone. 
                                    </p>

                                    <p class="text-muted">A deep reinforcement learning approach could also be attempted. As the goal 
                                        of this algorithm is to maximize the accumulated reward, the agent can ignore possible 
                                        limitations in hardware as it is solely identifying the best possible combination of actions 
                                        to determine the optimal policy. 
                                    </p>

                                    <p class="text-muted">The reward estimation could be directly correlated to the softmax predictions 
                                        of the model, wherein the predicted value of the desired tone is used as the reward for the 
                                        agent.
                                    </p>

                                    <h6>Final Word:</h6>
                                    <p class="text-muted">This is certainly a unique problem and this project is far from complete. 
                                        Although the results yielded accuracy lower than we had hoped, we were happy to have chosen
                                        a challenging project and learned from the time we spent working on it.  
                                    </p>


                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>

                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Gastrostomy Skin Level Device</h3>
                                        <p class="item-intro text-muted">Design for medical device for patients who require enteral 
                                            feeding focused on long-term durability and patient comfort</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/1.jpg" alt="..." />
                                       
                                    <h5>Introduction:</h5>
                                    <p class="text-muted">About half a million children and adults in the United States rely on feeding 
                                        tubes everyday. There are over 350 conditions and diseases in which enteral feeding may be necessary. 
                                        Enteral nutrition, or the method of delivering nutrition directly to the stomach 
                                        or small intestine, is required for anyone who cannot meet their nutritional needs by oral intake 
                                        but have a functional gastrointestinal tract. The use of enteral nutrition can be due to several factors. 
                                        Dysphagia, a difficulty to swallow, affects one-third of patients with Parkinson’s disease (PD).  
                                        Also, maintaining nutritional health can be difficult for people with cystic fibrosis (CF) as well as people 
                                        undergoing chemotherapy where a feeding tube can offer much needed nutrients and calories.
                                    </p>

                                    <p class="text-muted">Skin level devices, often referred to as “G-buttons” or “Gastrostomy buttons” 
                                        are medical devices designed for enteral feeding. They are inserted through a surgical incision 
                                        in the stomach called a gastrostomy and interface with a gastrostomy tube through which nutritional 
                                        formula flows. The quality of devices like the one shown in Figure 1 is crucial to ensuring patient 
                                        safety and comfort. Further research and development in this field will allow for customer feedback 
                                        of current products to be addressed. Innovation in the medical device industry in general is therefore 
                                        crucial in ensuring that the medical needs of all patients are met. 
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Schematic of Gastrostomy Skin Level Device</p>

                                    <h5>Design:</h5>
                                    <h6>Research and Benchmarking:</h6>
                                    <p class="text-muted">Following the human-centered design process I first researched and 
                                        subsequently defined the problem to provide a basis for development. As part of the research 
                                        phase, I developed a customer requirements table along with accompanying functional requirements, 
                                        technical interpretations, technical specifications, and metrics. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/15.png" alt="..." />
                                    <p class="item-intro text-muted">Table 1: Table of customer requirements each assigned a number 
                                        and color coded based on priority</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/16.png" alt="..." />
                                    <p class="item-intro text-muted">Table 2: Table with functional requirements of the product relating 
                                        to each of the customer requirements (Table 1) as well as a technical interpretation of the 
                                        requirement</p>

                                    <p class="text-muted">For product benchmarking, a thorough assessment of current technologies 
                                        that address similar customer requirements was then completed. 
                                        These technologies included products for purchase as well as several U.S. patents.
                                    </p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Current Products on the Market: AMT MiniONE® “Family 
                                        of G-Tubes”</p>
                                        
                                    <p class="text-muted">U.S. Patent Application Pub. No. 2006/0052752 was worth noting as it 
                                        provides a concept design for a gastrostomy button similar to the mentioned products but with 
                                        some improvements. The design offers a non-balloon internal bolster approach to allow for longer 
                                        wear time as gastrostomy buttons with silicone balloons must be replaced approximately every 
                                        three months to prevent rupture due to the concentration of hydrochloric acid (HCI) present in 
                                        gastric fluid. The design also includes two sets of silicone pleats that act as springs to 
                                        stabilize the port. An image displaying two views of the patented design are shown below in 
                                        Figure 3.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: SLD design involving silicone “pleat” stabilizers and 
                                        non-balloon internal bolster (U.S. Patent No. 8,951,232)</p>
                                    
                                    <h6>Prototyping:</h6>
                                    <p class="text-muted">The design process focused on addressing all customer needs especially those 
                                        involving patient safety and reliability. As part of the design process, several 3D printed 
                                        prototypes of the design were developed, tested, and iterated upon. One of the first prototypes
                                        is shown in Fig. 4. 
                                    </p>    

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Testing of 3D printed prototype for insertion ability 
                                        with thin metal rod</p>

                                    <p class="text-muted">Although the first prototype was partially validated through testing, a 
                                        device made of silicone rubber only addresses customer requirement 02 regarding reliability to 
                                        an extent. Gastric fluid is highly acidic and contains parietal cells that secrete hydrochloric 
                                        acid (HCI) to inactivate microorganisms (Heda et. al. 2021). Because of this, most gastrostomy 
                                        skin level devices made from medical grade silicone rubber must be removed approximately every 
                                        4 months to ensure the internal bolster does not degrade. However, to achieve long-term 
                                        durability, a material like polytetrafluoroethylene (PTFE) can be used as it is highly resistant 
                                        to HCI between concentrations of 0%-37% and is biocompatible.
                                    </p>

                                    <p class="text-muted">The first change made to the second prototype was reducing the thickness and width of the 
                                        panels that make up the internal bolster to reduce overall material so the panels would nicely 
                                        collapse when stretched. A schematic of the second prototype with labels is shown in Figure 5.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/6.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Labled Schematic of Second Prototype with labels (left) and 
                                        dimensions, in  mm (right)</p>

                                    <p class="text-muted">Other changes made to the design before building the second prototype addressed 
                                        customer requirements 01, safety, and 05, ability for the device to be low-profile.  The external 
                                        bolster was reduced to a diameter of 12.5 mm from 52.5 mm previously to reduce the overall 
                                        volume making the device more low-profile. The tether was increased in width to 2 mm as opposed 
                                        to 1 mm previously to reduce the risk of fracture.
                                    </p>

                                    <p class="text-muted">The cap was updated to feature custom threads to ensure they cannot interface with other 
                                        small-bore connectors that may be in a health care setting. This is a preventative step to 
                                        ensure patient safety and is outlined in ISO 80369-3: Small-bore connectors for liquids and 
                                        gases in healthcare applications — Part 3: Connectors for enteral applications, a series of 
                                        standards developed by the International Organization for Standardization to improve patient 
                                        safety with respect to small-bore connectors in healthcare settings.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/8.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Schematic of Second Prototype Cap with labels (left) and 
                                        dimensions, in mm (right)</p>
                                    
                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">After designing the new model in Solidworks, it was crucial to investigate 
                                        areas of stress concentration for this device as the internal bolster is subject to unique 
                                        force distributions in order to stretch out to fit through the stoma.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/10.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/11.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: FEA Simulation and Mechanical Properties of Internal 
                                        Bolster to Identify Stress Concentration Areas</p>
                                    
                                    <p class="text-muted">These simulations were used to find stress concentration areas only and 
                                        were not used to validate specific stress values
                                    </p>

                                    <p class="text-muted">To account for the stress concentration areas at the corners in the internal 
                                        bolster, fillets were added to the CAD model to distribute the stress. The second prototype was 
                                        3D printed from Formlabs Flexible 80A. Images of the second prototype as well as testing the 
                                        internal bolster are shown in Figure 8.
                                    </p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/12.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Testing of 3D printed prototype for insertion ability 
                                        with thin metal rod</p>

                                    <h6>Risk Analysis:</h6>
                                    <p class="text-muted">Lastly, an FMEA risk analysis table was developed to assess risks in the 
                                        proposed solution and serve as a tool to further update iterated designs.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/13.png" alt="..." />
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/gastrostomy/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: Failure Modes and Effects Analysis (FMEA) for Minimum 
                                        Viable Product (MVP)</p>

                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">Initial testing of my design showed that the prototype could address several of the customer 
                                        requirements I outlined at the beginning of the design process. However, by subjecting the prototype 
                                        to more rigorous testing conditions, like an environment that simulates gastric fluid, I could further 
                                        address potential challenges and refine the design. Fostering innovation within the medical device 
                                        industry allows technology to continually advance and meet the diverse and evolving medical needs of 
                                        all patients.
                                    </p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Algae Filtration for Kelp Growth</h3>
                                        <p class="item-intro text-muted">Abalone-inspired filtration system for addressing issue of diminishing kelp 
                                            forests off the Pacific coast of the U.S. and Mexico</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/1.png" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">
                                        Kelp forests are currently diminishing off the western coast of the United States and Mexico due to 
                                        warming waters and overfishing of white abalone. Kelp forests reduce wave energy therefore decreasing 
                                        the effects of coastal erosion which can wash away homes and the natural coastline. They are also home 
                                        to large populations of sea lions, starfish, and white abalone. The white abalone diet consists of algae, 
                                        which abalone clean from rock surfaces on the ocean floor providing an optimal place for kelp to grow. 
                                        Overfishing of white abalone therefore leads to algae overgrowth and threatens the health of kelp forests.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: (from left to right) Global kelp forest distribution, the 
                                        decrease in living shorelines contributes to coastal erosion and leaves coastal properties more 
                                        vulnurable to natural disasters, kelp bed recovery efforts include aquaculture of giant kelp
                                    </p>
                                    
                                    <h5>Objectives:</h5>
                                    <p class="text-muted">To address the overgrowth of algae due to the decreasing population of 
                                        white abalone, my team and I designed and built a prototype algae filtration system that mimics 
                                        the abilities of white abalone to clean rock surfaces of algae. The system would work in conjunction 
                                        with current efforts to reduce overfishing of white abalone, seed white abalone in restored kelp forests, 
                                        and require preventative action against climate change.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/2.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Full CAD Assembly of System</p>

                                    <h5>Design:</h5>
                                    <h6>Overview</h6>
                                    <p class="text-muted">Our algae filtration system is designed to be secured to 
                                        the rock surfaces in endangered kelp forests via a strong suction cup. It will then be able 
                                        to clean the rock surfaces of algae using a motor-powered brush located on the bottom of the 
                                        system. After the brush releases the algae from the rock surface, a motor-powered turbine will 
                                        produce a vacuum that will pull the now algae-filled water through the filter. Algae will 
                                        become trapped on the underside of the filter while clean water will exit through a top vent 
                                        in the external casing. I created all of the following CAD models and assemblies using 
                                        Solidworks.</p> 

                                    <h6>External Casing</h6>
                                    <p class="text-muted">The external casing is an 11 inch by 7 inch by 4.25 inch enclosure made of 
                                        Nylon 6/6 as this material has high corrosion resistance and behaves well in marine environments 
                                        for extended periods of time. This material is also easily injection moldable meaning it would 
                                        be relatively cheap to manufacture several hundred of these enclosures. The round brush and 
                                        heavy-duty suction cup can be seen in Fig. 2 as well as the openings in the bottom of the external 
                                        casing that lead to the filter. </p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/3.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Brush and suction cup of system</p>

                                    <h6>Internal Components</h6>
                                    <p class="text-muted">The external casing houses two 12 volt (V) DC motors, a turbine (Fig. 3.5), 
                                        two simple filters made of mesh, and a 12V 8 ampere-hour (A hr.) SLA battery, shown as the 
                                        large black box in Fig. 3. The turbine is press-fit onto the output shaft of one of the DC 
                                        motors while the other DC motor is connected to the round brush.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/4.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Interior components of system</p>

                                    <h6>Prototype:</h6>
                                    <p class="text-muted">The system prototype is a 3D printed 1:2 scaled version of the full system 
                                        and includes two DC motors, one suction cup, a 2 inch diameter round brush, and a turbine. The 
                                        battery does not fit inside the scaled enclosure so the DC motors were attached to a 12V power 
                                        adapter for the final prototype testing presentation. The DC motors are waterproofed with 
                                        Sugru™ and marine grease. Images of the prototype are shown below in Figs. 4-6.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/5.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: 1:2 Scaled prototype</p>

                                    <h6>Challenges:</h6>
                                    <p class="text-muted">I thought it would be most meaningful to our presentation to test the 
                                        prototype underwater but this meant the DC motors had to be waterproofed. I originally 
                                        thought sticking them in a food storage container would be adequate but containers I found 
                                        that fit the motors were very large and would require me to scale the enclosure to fit over 
                                        them. I then read this article from robotshop.com about waterproofing motors for marine use 
                                        using Sugru™ and marine grease.</p> 

                                    <p class="text-muted">I rolled out small cylinders of Sugru™ and pressed them into all gaps in 
                                        the external casing and then added marine grease to the output shaft, as 
                                        shown in Fig. 5, to ensure water does not enter between the casing and the shaft.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Waterproofing motor</p>   

                                    <h5>Evaluation:</h5>
                                    <p class="text-muted">The prototype was tested in a clear container filled with water. The 
                                        wires from each motor were clamped to the side of the container to ensure they do not come 
                                        in contact with the water. Each motor was connected to a 12V power adapter to test whether the 
                                        motors were fully functional after waterproofing.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: System running underwater</p>

                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">In conclusion, our prototype algae filtration system, designed to combat algae 
                                        overgrowth resulting from the declining white abalone population, has shown promising initial success. 
                                        However, we recognize the need for more comprehensive testing in an environment more similar to the 
                                        conditions of a real kelp forest to ensure its reliability. While the current results are 
                                        encouraging, our focus remains on refining the system through further testing to ultimately 
                                        develop a  self-sustaining system that fosters the growth of resilient and thriving kelp forests.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/algae/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Our poster for the project which includes a root cause analysis 
                                        of the problem, system requirements for the full system design, images and a description of our prototype, 
                                        and a technical drawing of the assembly
                                    </p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Globe Night Light</h3>
                                        <p class="item-intro text-muted">Design of a small, night light toy along with a 
                                            full manufacturing and assembly process for 500 units</p>
                                    </div>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/production.JPG" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">This project was part of Rensselaer 
                                        Polytechnic Institute's (RPI) Manufacturing Processes & Systems (MPS) course 
                                        focused on exposing students to common manufacturing techniques used in industry (e.g. plastic 
                                        injection molding, CNC machining, metalforming, and automation).</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/1.JPG" alt="..." />
                                    
                                    <h5>Objectives:</h5>
                                    <p class="text-muted">Each MPS team was tasked with designing a product and writing a technical 
                                        data package (TDP) outlining every step of the manufacturing process for the product with the intention that 
                                        if chosen for MPS II, the second half of the course, 500 units will be produced from raw materials in the RPI Manufacturing Innovation 
                                        Learning Lab (MILL). The TDP includes all information for the product including product 
                                        component descriptions, a bill of materials, all technical drawings for product components and 
                                        all assembly fixtures, vises, and molds used in the manufacturing process, and detailed 
                                        manufacturing forms including simulations run on each part using software like Mastercam and 
                                        Autodesk Moldflow. The TDP also includes a detailed description of the assembly process 
                                        including part transfer and quality control, and a brief description of how all 500 of the 
                                        product will be packaged.</p>

                                    <h5>Design:</h5>
                                    <h6>Overview</h6>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Exploded View of Final Design with Labels</p> 

                                    <h6>Baseplate</h6>
                                    <p class="text-muted">The base plate adds weight to the bottom of the assembly such that the user can spin 
                                        the globe without it tipping over. I designed the baseplate with a 3 inch diameter and 0.25 inch height. 
                                        It also features a clearance hole for the 6-32 threaded rod and a 0.05 inch chamfer.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Base plate of product designed for CNC machining from Aluminum 6061</p> 

                                    <h6>Base Body</h6>
                                    <p class="text-muted">The base body is the housing for the 9V battery and power switch. I designed it with 
                                        constant wall thickness of 0.07 inches and a 3 degree draft on all vertical surfaces to abide by 
                                        plastic injection molding design parameters.</p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Base body designed for plastic injection molding from 
                                        ABS plastic</p>

                                    <p class="text-muted">The base body also has 4 bosses extruded from its underside such that the 
                                        base connector disk can be screwed into the bottom of the base body.</p> 

                                    <p class="text-muted">The “Rensselaer” text is CNC machined into the part post-plastic injection 
                                        molding to avoid use of a side-action cam in the PIM mold.</p> 

                                    <h6>Base Connector</h6>
                                    <p class="text-muted">The base connector interfaces with the bottom surface of the base body and 
                                        is secured by four 4-40 flat head screws. The connector also has a thickness of 0.07 inches 
                                        such that the mold for it can be implemented into the same mold as the base body. A center 
                                        boss with a tapped hole secures the base connector to the threaded rod which extends the 
                                        entire height of the assembly.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Base connector designed for plastic injection molding</p>

                                    <h6>Globe Connector Disk</h6>
                                    <p class="text-muted">The globe connector disk is the interface between both vacuum-formed 
                                        globe halves upon which they are heat staked. The two outermost pins exist for the purpose 
                                        of heat staking. Similar to the base body and base connector, the globe connector has a 
                                        thickness of 0.07 inches such that it can be produced using the same mold as the other two 
                                        parts.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/6.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Globe connector designed for plastic injection molding</p>

                                    <p class="text-muted">An off-centered boss with a 2-56 tapped hole was added to secure the 
                                        circular PCBa as shown in Fig. 6. Small pins were also added to act as support pins 
                                        for the PCBa and lift it off the main surface of the globe connector.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Globe connector supporting circular PCBa</p>

                                    <p class="text-muted">Our project was selected by the MPS instructors for Part 2 of the class, which 
                                        involved producing 500 units of the globe toy. I graduated from RPI in December 2021, so I wasn’t 
                                        part of the production phase, but my teammates who stayed sent photos of the completed toys. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/production.JPG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Globe production line</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/globe/team.JPG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: My awesome team</p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>CMU Array Benchtop Testing Setup</h3>
                                        <p class="item-intro text-muted">Setup for positioning 3D-printed neural implant probes 
                                            for stimulation testing</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/1.jpg" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">I designed and built this testing setup for the Panat Lab at Carnegie Mellon 
                                        University. In a collaborative effort, the lab is developing a microelectrode array, the 
                                        <a href="https://www.science.org/doi/full/10.1126/sciadv.abj4853">CMU Array</a>, fabricated from 
                                        depositing metal nanoparticles onto a substrate. The long, narrow shanks are then sintered to create 
                                        conductive paths for bioelectric signals as shown in Fig. 1.
                                    </p>

                                    <div class="row text-center">
                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/4.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 1: Microelectrode array fabricated from 
                                                depositing metal nanoparticles onto a substrate</p>
                                        </div>

                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/5.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 2: Array can be tested in vivo using neural activity 
                                                from anesthetized mice but removing unnecessary animal testing is ideal</p> 
                                        </div>

                                        <div class="col-md-4">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/6.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 3: Benchtop testing allows for neural recording sessions 
                                                using a stimulating electrode to simulate neural activity</p>
                                        </div>
                                    </div>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">The array can be tested in vivo using neural activity (Fig. 2) from anesthetized 
                                        mice but benchtop testing allows for neural recording sessions using a stimulating electrode 
                                        to simulate neural activity (Fig. 3) which can reduce variability between experiments and eliminates 
                                        the need for live animals. Before designing the setup, I outlined 3 main objectives:</p>

                                    <div class="row text-left">
                                        <div class="col-md-4">
                                            <h6>1.</h6>
                                            <p class="text-muted">Reduce movement of the electrode and substrate from outside disruption 
                                                    by securing them in fixtures designed for their unique geometry.</p>
                                        </div>

                                        <div class="col-md-4">
                                            <h6>2.</h6>
                                            <p class="text-muted">Increase accuracy when moving electrode and substrate by 
                                                securing their fixtures to translational stages.</p>
                                        </div>
                                        
                                        <div class="col-md-4">
                                            <h6>3.</h6>
                                            <p class="text-muted">Reduce difficulty of clearing the optical table by securing all hardware 
                                                to an optical breadboard that is screwed into the table and can easily be switched out 
                                                when needed.</p>
                                        </div>
                                    </div>

                                    <h5>Design:</h5>
                                    <p class="text-muted">To address the first objective, I designed a subassembly for the probe. Two 
                                        small translational stages (Thorlabs DT12) are used: one is mounted to the elevated stage and 
                                        allows movement of the arm in the x, and one is mounted to the arm and allows movement of 
                                        clamped substrate in the z.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Probe Subassembly</p>

                                    <p class="text-muted">The substrate clamp restricts all movement of the substrate and ensures the 
                                        electrodes are in the same location for every test. The bottom part of the clamp is permanantly 
                                        secured to the translational stage while the top part can easily be removed via two captive 
                                        screws.  
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: Substrate Clamp</p>


                                    <p class="text-muted">The electrode needed to intercept with the probes at a certain angle and 
                                        was therefore mounted onto a 3D printed part and clamped into place in a similar way to the 
                                        substrate: via a captive screw. I press fit an insert into the bottom part of the electrode 
                                        clamp to ensure the threads wouldn't strip after many uses. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Electrode clamp shown empty with slot channel for electrode 
                                        wire (left), placing the wire into the slot (middle), and a fully clamped electrode (right) 
                                    </p>

                                    <p class="text-muted">The electrode clamp extends from the front camera via a strut channel as 
                                        shown in Fig. 7. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/9.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Full assembly with side camera, front camera with electrode
                                        mount, and probe subassembly 
                                    </p>

                                    <p class="text-muted">To mount the cameras, I first designed a 90 degree bracket to secure the 
                                        3-axis translational stages (Thorlabs DT12XYZ) to the aluminum breadboard. Then, I designed 
                                        another bracket to mount the round, lens mounting clamp (Infinity Large Mounting Clamp) to 
                                        the stages. This would allow each camera three degrees of freedom and a secure and minimally
                                        invasive way to mount the cameras. 
                                    </p>

                                    <p class="text-muted">All four of the brackets were first cut from a 1 ft. Aluminum 6061 L bar
                                        and manually machined to obtain the cutout for the stages and necessary holes and 
                                        countersinks as shown in Fig. 8.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 8: Cutting L Bar and Machining Camera Mounts  
                                    </p>

                                    <p class="text-muted">The brackets are shown assembled onto the aluminum breadboard in Fig. 9.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/13.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 9: Stage bracket (left) supports entire camera assembly 
                                        including XYZ stages, camera, and camera mount, front camera bracket (middle) supports camera
                                        mount and strut channel, and side camera bracket (right) supports camera and camera mount  
                                    </p>

                                    <p class="text-muted">I 3D printed all components of the probe subassembly, press-fit inserts 
                                        into the substrate clamp (Fig. 11) and assembled them onto the laser-cut acrylic stage 
                                        elevated by hex standoffs. The clear container for PBS is a simple storage container from 
                                        The Container Store. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/14.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 10: Assembled Probe Subassembly</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/15.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 11: Assembled Substrate Clamp</p>
                                    
                                    <h5>Evaluation:</h5>
                                    
                                    <p class="text-muted">Camera positioning is crucial for the test as the XYZ stage also moves the 
                                        electrode wire for stimulating each probe. Example photos I took to test camera positioning
                                        are shown in Fig. 12.
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/electrode/16.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 12: Positioning Cameras</p>
                                    
                                    <p class="text-muted">Unfortunately, my internship in the lab ended before I could conduct further testing, 
                                        but potential future evaluations could involve rigorous testing of long-term stability, real-time 
                                        data processing capabilities, and correlation studies with in vivo experiments.</p>
                                    
                                    <h5>Conclusion:</h5>
                                    <p class="text-muted">
                                        The microelectrode array testing setup I designed and built allows for benchtop testing with a stimulating 
                                        electrode, reducing variability and eliminating the need for live animals.  
                                    </p>
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Redwood Desk Organizer</h3>
                                        <p class="item-intro text-muted">Adjustable length desk organizer made of repurposed redwood</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/1.jpg" alt="..." />
                                    
                                    <h5>Background:</h5>
                                    <p class="text-muted">At one of my internships I sat at a desk right next to an aisleway which sometimes
                                        got very busy and distracting. I had this idea that I wanted some kind of separator that also doubled 
                                        as an organizer for some of the tools I used everyday. None of the desk organizers I could find on 
                                        Amazon were quite what I wanted so when I got home I quickly modeled my idea.</p>

                                    <h5>Objectives:</h5>
                                    <p class="text-muted">I wanted the design to have an elevated surface for my plant to sit on. 
                                        This, along with compartments to organize my desk items, would act as a separator 
                                        between my desk and the aisleway next to my desk. I also wanted the freedom to adjust the size of 
                                        the organizer in case I ended up needing more space on the desktop.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Initial CAD model</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/3.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Visualization of two sliding compartments</p>
                                    
                                    <h5>Approach:</h5>
                                    <p class="text-muted">I used repurposed redwood from an old play structure in my backyard that my dad 
                                        had recently dissasembled. We first rip cut large pieces of the wood such that they were the correct width
                                        and depth for the boards I needed. I then used the table saw to cut grooves in the vertical standing 
                                        boards for a through dado joint. These joints would better support the shelving than a butt joint.</p>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/4.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Joint used to support shelving</p>
                                    
                                    <p class="text-muted">I then assempled all the pieces applying a generous amount of wood glue between each,
                                        and clamped them together. After about 48 hours, I sanded the entire strucutre including any excess wood glue 
                                        from the joints and applied a polyurethane finish to protect the wood from water that might leak from the plant.</p>    
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/5.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Applying a wipe-on polyurethane finish to protect the wood from water</p>
                                    
                                    <p class="text-muted">I am overall very satisfied with how it turned out and used it for the rest of my internship.
                                    </p>    
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/redwood/7.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 5: The completed organizer on my desk where I use it to store tools I used daily 
                                        and display my very quickly growing plant
                                    </p>
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal10" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Neural Implant Imaging Station</h3>
                                        <p class="item-intro text-muted">Automated imaging setup for documentation of micron-scale implant threads</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/1.jpg" alt="..." />
                                    
                                    <h5>Background</h5>
                                    <p class="text-muted">This project was part of my mechanical engineering internship at Neuralink. The Neuralink 
                                        implant contains micon-scale threads that are inserted into the motor cortex, a region of the frontal lobe 
                                        that is responsible for planning and executing motor movements, via a high-precision surgical robot. The 
                                        robot uses a needle that sews the threads into the brain by first inserting onto a small loop on the end of 
                                        each thread.
                                    </p>

                                    <h5>Objectives</h5>
                                    <p class="text-muted">The purpose of the implant imaging station is to take a high resolution image of the implant 
                                        loops to ensure they are all located on the same plane and in a location in space that the robot is able to pick 
                                        them up.  The station also takes two images of the full implant as a visual quality check and documentation 
                                        before packaging.
                                    </p>
                                    
                                    <h5>Approach</h5>
                                    <p class="text-muted">I designed the assembly in Solidworks. The program moves a 2X magnification camera along a 
                                        translational stage in small increments, taking an image at each step. These images are stitched into a single 
                                        high-resolution view, followed by two full-field images of the implant, all of which are automatically uploaded 
                                        to a database. The system automated four previously manual steps of the implant production process.
                                    </p> 

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Full assembly</p>
                                    
                                    <h5>Components</h5>
                                    <h6>Baseplates</h6>
                                    <p class="text-muted">The baseplate serves as the main surface that all components mount to. I designed the part for 
                                        CNC machining and it is made from Aluminum 6065. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Baseplate designed for CNC machining</p>

                                    <p class="text-muted">The baseplate includes countersunk mounting holes to fasten the top and bottom plates together, 
                                        along with integrated tapped holes for securing all hardware. Edge cutouts were added for cable management.</p>

                                    <h6>Camera Subassembly</h6>
                                    <p class="text-muted">The USB camera subassembly includes a motorized translational stage, camera tower, mount, manual 
                                        focus stage, USB camera, 1.25X objective, and Coaxial LED spot light. 
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/4.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Coaxial (brightfield) lighting was used 
                                        to capture the highly reflective polyamide loops.</p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/5.PNG" alt="..." />
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/6.PNG" alt="..." />
                                        </div>
                                    </div>
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/9.PNG" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: The USB camera mount attaches the camera to the manual 
                                        translational stage, allowing for fine vertical focus adjustment.</p>

                                    <h6>Enclosure</h6>
                                    <p class="text-muted">The enclosure provides a white imaging backdrop and controlled lighting via a 
                                        mounted ring light. I laser-cut the panels from white acrylic and assembled the enclosure, with 
                                        cutouts for the moving camera tower and fixed board cameras.     
                                    </p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/7.PNG" alt="..." />
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/8.PNG" alt="..." />
                                        </div>
                                    </div>
                                    <p class="item-intro text-muted">Fig. 5: Enclosure designed for laser cutting and provides white 
                                        backdrop and controlled lightning for all images</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/10.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 6: Angled board camera mount orients board camera at 45 degrees
                                        for full images of the implant</p>

                                    <h5>System Setup</h5>
                                    <p class="text-muted">I installed the fully assembled imaging station on the production line after 
                                        recieving the machined parts (baseplates and camera tower), integrated the motorized stage and 
                                        cameras with a PC, and developed the control program to capture, stitch, and automatically upload 
                                        all required images to the database.     
                                    </p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/implant/1.jpg" alt="..." />
                                    <p class="item-intro text-muted">Fig. 7: Imaging station on implant production line</p>

                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Portfolio item modal popup-->
        <div class="portfolio-modal modal fade" id="portfolioModal11" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/img/close-icon.svg" alt="Close modal" /></div>
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <div class="modal-body">
                                    
                                    <!-- Project details-->
                                    <div class="text-center">
                                        <h3>Diced Wafer Cleaning Fixture</h3>
                                        <p class="item-intro text-muted">Fixture designed for improved solvent flow and efficiency during chemical baths, megasonic cleaning, and vapor drying</p>
                                    </div>
                                    
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/1.png" alt="..." />
                                    
                                    <h5>Background</h5>
                                    <p class="text-muted">This project was part of my mechanical engineering internship at Neuralink. The existing cleaning fixture, used to clean the 
                                        microfabricated array threads, was heavy and designed for horizontal processing, which limited cleaning efficiency in chemical baths, 
                                        sonication, and vapor drying. Horizontal orientation reduces particle removal during megasonic cleaning and hinders proper drainage during vapor drying. 
                                        The microfabrication team also found the fixture difficult to handle due to it's size and weight.</p>

                                    <h5>Objectives</h5>
                                    <p class="text-muted">Redesign the cleaning fixture for vertical processing to improve particle removal and solvent flow, reduce fixture weight for easier 
                                        handling, and maintain precise support and minimal contact for delicate wafers during chemical and sonic cleaning steps.</p>
                                    
                                    <h5>Approach</h5>
                                    <p class="text-muted">I redesigned the fixture with sub-flush faces at the chip and loop regions, and added 45° drafted edges on cover cutouts for improved 
                                        solvent flow, and added bosses for support with minimal contact also improving flow.</p>  
                                        
                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/2.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 1: Features for increased solvent flow</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/8.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 2: Redesigned fixture</p>

                                    <p class="text-muted">The fixtures were machined from lightweight PEEK, reducing weight and making them easier for the microfabrication team to handle while 
                                        maintaining durability and chemical resistance.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/3.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 3: Fixture cover with press-fit alignment pins and PTFE inserts installed for clamping the wafer</p>
                                    
                                    <p class="text-muted">The previous design also used steel thumbscrews, which left particles on the wafer with repeated use. I designed a custom quarter-turn 
                                        fastener machined from PEEK with a steel press-fit pin to secure the top and bottom of the fixture. After testing 20 cycles of fastening and unfastening, 
                                        the quarter-turn fastener produced significantly fewer particles on an empty wafer compared to the steel screws.</p>

                                    <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/5.png" alt="..." />
                                    <p class="item-intro text-muted">Fig. 4: Assembly of fixture with custom fastener for reduced particle generation</p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/4.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 5: Thumbscrew particle generation</p>
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/6.png" alt="..." />
                                            <p class="item-intro text-muted">Fig. 6: Reduced particle generation with custom PEEK fastener</p>
                                        </div>
                                    </div>

                                    <p class="text-muted">I ordered all machined components for a batch of 10 fixtures and fully assembled each unit. My internship ended shortly after, but 
                                        I delivered the set ready for immediate use, ensuring a smooth handoff.</p>

                                    <div class="row text-center">
                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/7.png" alt="..." />
                                        </div>

                                        <div class="col-md-6">
                                            <img class="img-fluid d-block mx-auto" src="assets/img/portfolio/wafer/1.png" alt="..." />
                                        </div>
                                    </div>
                                    <p class="item-intro text-muted">Fig. 7: Assembly of 10 units for production use</p>
                                    
                                    <div class="text-center">
                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>

    </body>
</html>
